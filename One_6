#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Module: brm_tool_data_access.py
Description:
  Core Data Access and Business Logic module for the BRM Tool.
  This module includes:
    • Database helper functions (fetching, audit logging).
    • Functions to detect operation types and parse SQL dependencies (with deep integration of decision tables).
    • Core rule execution functions with dry‑run simulation.
    • Advanced BFS‑based execution methods that log impacted record counts, success/failure details,
      and cascade processing for conflicts, composite rules, and global‑critical links.
    • CRUD functions for adding, updating, deactivating, and deleting rules.
    • Simulation functions (chain simulation and custom group simulation) that capture detailed logs.
    
All functions are production‑ready, robustly handle errors, and follow standard naming conventions.
  
Dependencies:
  • pyodbc, sqlparse, logging, json, math, re, datetime, collections.
  
Usage:
  Import this module as “brm_tool_data_access” and use its functions to perform rule execution,
  simulations, and CRUD operations.
"""

import sys
import json
import math
import re
import logging
from datetime import datetime, timedelta
from collections import deque

import pyodbc
import sqlparse
from sqlparse.sql import Identifier, IdentifierList, Parenthesis
from sqlparse.tokens import Keyword, DML

# Set up module logger
logger = logging.getLogger("brm_tool_data_access")
logger.setLevel(logging.DEBUG)
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(name)s:%(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

# =============================================================================
# DATABASE HELPER FUNCTIONS
# =============================================================================
def fetch_all_dict(cursor):
    """
    Returns all fetched rows as list of dictionaries.
    """
    rows = cursor.fetchall()
    if cursor.description:
        cols = [d[0] for d in cursor.description]
        return [dict(zip(cols, row)) for row in rows]
    return rows

def fetch_one_dict(cursor):
    """
    Returns one fetched row as a dictionary.
    """
    row = cursor.fetchone()
    if row and cursor.description:
        cols = [d[0] for d in cursor.description]
        return dict(zip(cols, row))
    return None

def insert_audit_log(conn, action, table_name, record_id, actor, old_data, new_data):
    """
    Inserts an audit log entry into BRM_AUDIT_LOG.
    Old and new data are stored as JSON.
    """
    try:
        c = conn.cursor()
        c.execute("""
            INSERT INTO BRM_AUDIT_LOG(
                ACTION, TABLE_NAME, RECORD_ID, ACTION_BY,
                OLD_DATA, NEW_DATA, ACTION_TIMESTAMP
            )
            VALUES (?, ?, ?, ?, ?, ?, GETDATE())
        """, (
            action,
            table_name,
            str(record_id) if record_id else None,
            actor,
            json.dumps(old_data) if old_data else None,
            json.dumps(new_data) if new_data else None
        ))
        conn.commit()
        logger.info(f"Audit log inserted for {action} on {table_name} record {record_id}.")
    except Exception as ex:
        logger.error(f"Error inserting audit log: {ex}")

# =============================================================================
# OPERATION DETECTION & SQL DEPENDENCY PARSING
# =============================================================================
def detect_operation_type(rule_sql: str, decision_table_id=None) -> str:
    """
    Returns one of: INSERT/UPDATE/DELETE/SELECT/DECISION_TABLE/OTHER.
    If rule_sql is empty but decision_table_id is provided, returns DECISION_TABLE.
    """
    if (not rule_sql.strip()) and decision_table_id:
        return "DECISION_TABLE"
    txt = rule_sql.strip().upper()
    if txt.startswith("INSERT"):
        return "INSERT"
    elif txt.startswith("UPDATE"):
        return "UPDATE"
    elif txt.startswith("DELETE"):
        return "DELETE"
    elif txt.startswith("SELECT"):
        return "SELECT"
    return "OTHER"

def parse_sql_dependencies(sql_text: str) -> dict:
    """
    Parses the SQL text using sqlparse to extract:
      - List of table references (including subselects and CTEs)
      - CTE table information
      - Alias mapping and referenced columns.
    Returns a dictionary with keys: 'tables', 'cte_tables', 'alias_map', 'columns'.
    """
    try:
        statements = sqlparse.parse(sql_text)
    except Exception as ex:
        logger.error(f"SQL parse error: {ex}")
        return {"tables": [], "cte_tables": [], "alias_map": {}, "columns": []}

    all_tables = []
    cte_info = []
    alias_map = {}
    columns = []
    for stmt in statements:
        ctes = _extract_with_clauses(stmt)
        for cte_name, cte_refs in ctes.items():
            cte_info.append((cte_name, cte_refs))
        main_refs, main_alias = _extract_main_from(stmt.tokens, set(ctes.keys()))
        all_tables.extend(main_refs)
        alias_map.update(main_alias)
        col_refs = _extract_columns(stmt)
        columns.extend(col_refs)
    unique_tables = list({t for t in all_tables})
    return {
        "tables": unique_tables,
        "cte_tables": cte_info,
        "alias_map": alias_map,
        "columns": columns
    }

# Helper functions for SQL parsing (internal)
def _extract_with_clauses(statement):
    cte_map = {}
    tokens = list(statement.tokens)
    i = 0
    while i < len(tokens):
        tk = tokens[i]
        if tk.ttype is Keyword and tk.value.upper() == "WITH":
            i += 1
            i = _parse_cte_block(tokens, i, cte_map)
            continue
        i += 1
    return cte_map

def _parse_cte_block(tokens, i, cte_map):
    while i < len(tokens):
        tk = tokens[i]
        if isinstance(tk, Identifier):
            cte_name = tk.get_real_name()
            i += 1
            i = _parse_cte_as_clause(tokens, i, cte_name, cte_map)
        elif tk.ttype is Keyword and tk.value.upper() in ("SELECT", "INSERT", "UPDATE", "DELETE"):
            return i
        else:
            i += 1
    return i

def _parse_cte_as_clause(tokens, i, cte_name, cte_map):
    while i < len(tokens):
        tk = tokens[i]
        val = tk.value.upper() if tk.ttype else ""
        if val == "AS":
            i += 1
            if i < len(tokens):
                sub = tokens[i]
                if isinstance(sub, Parenthesis):
                    sub_refs = _extract_subselect_tokens(sub.tokens)
                    cte_map[cte_name] = sub_refs
                    i += 1
                    return i
        else:
            i += 1
    return i

def _extract_subselect_tokens(tokens):
    results = []
    from_seen = False
    i = 0
    while i < len(tokens):
        tk = tokens[i]
        if tk.is_group and _is_subselect(tk):
            results.extend(_extract_subselect_tokens(tk.tokens))
        if tk.ttype is Keyword:
            upv = tk.value.upper()
            if upv in ("FROM", "JOIN", "INNER JOIN", "LEFT JOIN", "RIGHT JOIN", "FULL JOIN"):
                from_seen = True
            else:
                from_seen = False
        if from_seen:
            if isinstance(tk, IdentifierList):
                for ident in tk.get_identifiers():
                    st = _parse_identifier(ident, set())
                    results.append((st[0], st[1], st[2], True))
            elif isinstance(tk, Identifier):
                st = _parse_identifier(tk, set())
                results.append((st[0], st[1], st[2], True))
        i += 1
    return results

def _is_subselect(token):
    if not token.is_group:
        return False
    for sub in token.tokens:
        if sub.ttype is DML and sub.value.upper() == "SELECT":
            return True
    return False

def _extract_main_from(tokenlist, known_cte_names):
    results = []
    alias_map = {}
    tokens = list(tokenlist)
    from_seen = False
    i = 0
    while i < len(tokens):
        tk = tokens[i]
        if tk.is_group and _is_subselect(tk):
            results.extend(_extract_subselect_tokens(tk.tokens))
        if tk.ttype is Keyword:
            upv = tk.value.upper()
            if upv in ("FROM", "JOIN", "INNER JOIN", "LEFT JOIN", "RIGHT JOIN", "FULL JOIN"):
                from_seen = True
            else:
                from_seen = False
        if from_seen:
            if isinstance(tk, IdentifierList):
                for ident in tk.get_identifiers():
                    st = _parse_identifier(ident, known_cte_names)
                    results.append(st)
                    if st[2]:
                        alias_map[st[2]] = (st[0], st[1])
            elif isinstance(tk, Identifier):
                st = _parse_identifier(tk, known_cte_names)
                results.append(st)
                if st[2]:
                    alias_map[st[2]] = (st[0], st[1])
        i += 1
    return (results, alias_map)

def _parse_identifier(ident, known_cte_names):
    alias = ident.get_alias()
    real_name = ident.get_real_name()
    schema_name = ident.get_parent_name()
    if real_name and real_name.upper() in (n.upper() for n in known_cte_names):
        return (None, f"(CTE) {real_name}", alias, False)
    return (schema_name, real_name, alias, False)

def _extract_columns(statement):
    results = []
    tokens = list(statement.tokens)
    i = 0
    while i < len(tokens):
        tk = tokens[i]
        if tk.ttype is DML:
            word = tk.value.upper()
            if word == "SELECT":
                columns = _parse_select_list(tokens, i+1)
                for c_ in columns:
                    results.append((c_, False, True))
            elif word in ("INSERT", "UPDATE"):
                columns = _parse_dml_columns(tokens, i, word)
                for c_ in columns:
                    results.append((c_, True, False))
        i += 1
    return results

def _parse_select_list(tokens, start_idx):
    columns = []
    i = start_idx
    while i < len(tokens):
        tk = tokens[i]
        if tk.ttype is Keyword and tk.value.upper() in ("FROM", "JOIN", "WHERE", "GROUP", "ORDER", "UNION", "INTERSECT"):
            break
        if isinstance(tk, IdentifierList):
            for ident in tk.get_identifiers():
                nm = ident.get_name()
                if nm and nm.upper() not in ("DISTINCT", "TOP", "ALL"):
                    columns.append(nm)
        elif isinstance(tk, Identifier):
            nm = tk.get_name()
            if nm and nm.upper() not in ("DISTINCT", "TOP", "ALL"):
                columns.append(nm)
        i += 1
    return columns

def _parse_dml_columns(tokens, start_idx, dml_word):
    columns = []
    if dml_word == "INSERT":
        i = start_idx
        while i < len(tokens):
            tk = tokens[i]
            if tk.is_group and isinstance(tk, Parenthesis):
                for st in tk.tokens:
                    if isinstance(st, IdentifierList):
                        for ident in st.get_identifiers():
                            columns.append(ident.get_name())
                    elif isinstance(st, Identifier):
                        columns.append(st.get_name())
                return columns
            i += 1
    elif dml_word == "UPDATE":
        i = start_idx
        found_set = False
        while i < len(tokens):
            tk = tokens[i]
            if tk.ttype is Keyword and tk.value.upper() == "SET":
                found_set = True
                i += 1
                columns.extend(_parse_update_set_list(tokens, i))
                break
            i += 1
    return columns

def _parse_update_set_list(tokens, start_i):
    columns = []
    i = start_i
    while i < len(tokens):
        tk = tokens[i]
        if tk.ttype is Keyword and tk.value.upper() in ("WHERE", "FROM"):
            break
        if isinstance(tk, Identifier):
            columns.append(tk.get_name())
        i += 1
    return columns

# =============================================================================
# CORE RULE EXECUTION & SIMULATION FUNCTIONS
# =============================================================================
def run_single_rule_transaction(conn, rule_info, is_dry_run=False):
    """
    Executes a single rule within a transaction.
    For DECISION_TABLE type, returns stub pass.
    For normal rules, executes the SQL and checks if the first column of the first row equals 1.
    Captures and returns:
       (success_flag, message, record_count)
    Also logs the number of impacted records.
    """
    op_type = rule_info.get("OPERATION_TYPE", "OTHER")
    if op_type == "DECISION_TABLE":
        dt_id = rule_info.get("DECISION_TABLE_ID")
        msg = f"DecisionTable {dt_id} executed in simulation mode (stub PASS)."
        logger.info(msg)
        return (True, msg, 1)

    sql_text = rule_info.get("RULE_SQL", "").strip()
    c = conn.cursor()
    try:
        c.execute("BEGIN TRANSACTION")
        c.execute(sql_text)
        rows = c.fetchall()
        rec_count = len(rows)
        # Assume rule passes if first cell equals 1 (or if no rows, we consider it as PASS)
        if rows and rows[0] and rows[0][0] == 1:
            success = True
            msg = f"Rule executed successfully; returned {rows[0][0]} with {rec_count} row(s)."
        else:
            success = True if not rows else False
            msg = "No rows returned; considered PASS." if not rows else f"Returned {rows[0][0]}; rule FAIL."
        if is_dry_run or not success:
            c.execute("ROLLBACK")
        else:
            c.execute("COMMIT")
    except Exception as ex:
        c.execute("ROLLBACK")
        success = False
        rec_count = 0
        msg = f"Error: {ex}"
        logger.error(f"Rule ID {rule_info.get('RULE_ID')} execution error: {ex}")
    logger.info(f"Rule ID {rule_info.get('RULE_ID')}: {msg} (Records affected: {rec_count})")
    return (success, msg, rec_count)

def advanced_bfs_execute(conn):
    """
    Advanced BFS-based execution of rules.
    Builds relationships (child rules, global-critical links, conflicts, composites)
    and executes rules in BFS order. If a rule fails and is critical, skips its subtree.
    Logs execution outcomes and captures impacted record counts.
    Returns a tuple: (executed_rule_ids, skipped_rule_ids)
    """
    adjacency, roots, _ = load_rule_relationships(conn)
    rule_lookup = get_all_rules_map(conn)
    executed = []
    skipped = set()
    queue = list(roots)
    while queue:
        rid = queue.pop(0)
        if rid in skipped:
            continue
        if rid not in rule_lookup:
            skipped.add(rid)
            continue
        info = rule_lookup[rid]
        success, msg, rec_count = run_single_rule_transaction(conn, info, is_dry_run=False)
        insert_rule_execution_log(conn, rid, success, msg, rec_count)
        if success:
            executed.append(rid)
            if rid in adjacency:
                for child in adjacency[rid]:
                    if child not in skipped:
                        queue.append(child)
        else:
            # If rule fails and is critical, skip its descendants.
            if info.get("CRITICAL_RULE", 0) == 1:
                skip_all_descendants(rid, adjacency, skipped)
            skipped.add(rid)
    return (executed, skipped)

def dry_run_bfs(conn, start_rule_ids):
    """
    Executes a dry-run BFS from the specified starting rule IDs.
    Always rolls back changes.
    Returns (executed_rule_ids, skipped_rule_ids) along with a detailed log message.
    """
    adjacency, _, _ = load_rule_relationships(conn)
    rule_lookup = get_all_rules_map(conn)
    executed = []
    skipped = set()
    queue = list(start_rule_ids)
    while queue:
        rid = queue.pop(0)
        if rid in skipped:
            continue
        if rid not in rule_lookup:
            skipped.add(rid)
            continue
        info = rule_lookup[rid]
        success, msg, rec_count = run_single_rule_transaction(conn, info, is_dry_run=True)
        logger.info(f"[Dry-Run] Rule {rid}: {msg} (Impacted: {rec_count} row(s))")
        if success:
            executed.append(rid)
            if rid in adjacency:
                for child in adjacency[rid]:
                    if child not in skipped:
                        queue.append(child)
        else:
            skip_all_descendants(rid, adjacency, skipped)
            skipped.add(rid)
    return (executed, skipped)

def execute_rule_simulation(conn, rule_id):
    """
    Executes a single rule in dry-run mode and returns a detailed message including
    the number of records impacted.
    """
    c = conn.cursor()
    c.execute("SELECT * FROM BRM_RULES WHERE RULE_ID=?", (rule_id,))
    row = c.fetchone()
    if not row:
        msg = f"Rule {rule_id} not found."
        logger.error(msg)
        return (False, msg, 0)
    colnames = [desc[0] for desc in c.description]
    rule_info = dict(zip(colnames, row))
    success, msg, rec_count = run_single_rule_transaction(conn, rule_info, is_dry_run=True)
    full_msg = f"Dry-run for Rule {rule_id}: {'PASS' if success else 'FAIL'}; {msg}; {rec_count} record(s) impacted."
    logger.info(full_msg)
    return (success, full_msg, rec_count)

# =============================================================================
# CRUD FUNCTIONS FOR RULES
# =============================================================================
def get_all_rules_map(conn):
    """
    Returns a dictionary mapping RULE_ID to full rule data from BRM_RULES.
    """
    c = conn.cursor()
    c.execute("SELECT * FROM BRM_RULES")
    rows = c.fetchall()
    colnames = [desc[0] for desc in c.description]
    rule_map = {}
    for row in rows:
        rule_map[row[0]] = dict(zip(colnames, row))
    return rule_map

def add_rule(conn, rule_data, created_by, user_group):
    """
    Adds a new rule into BRM_RULES.
    Performs duplicate checks, SQL dependency parsing, table-level permission validation,
    and sets the lifecycle to DRAFT.
    After insertion, it calls create_multistep_approvals to initiate approval flow.
    Returns the new rule ID.
    """
    c = conn.cursor()
    # Duplicate name check within the same owner group.
    c.execute("SELECT RULE_ID FROM BRM_RULES WHERE OWNER_GROUP=? AND RULE_NAME=?",
              (rule_data["OWNER_GROUP"], rule_data["RULE_NAME"].strip()))
    if c.fetchone():
        raise ValueError("A rule with the same name already exists in the owner group.")
    
    # Duplicate SQL check.
    new_sql = rule_data.get("RULE_SQL", "").strip()
    if new_sql:
        c.execute("SELECT RULE_ID FROM BRM_RULES WHERE RULE_SQL=?", (new_sql,))
        dup = c.fetchone()
        if dup:
            raise ValueError("Another rule with the same SQL already exists.")
    
    # Set initial lifecycle state.
    rule_data["LIFECYCLE_STATE"] = "DRAFT"
    op_type = detect_operation_type(new_sql, rule_data.get("DECISION_TABLE_ID"))
    rule_data["OPERATION_TYPE"] = op_type

    # Parse SQL dependencies for table permission checks.
    parse_info = {}
    col_op = "READ"
    if op_type not in ("DECISION_TABLE", "OTHER") and new_sql:
        parse_info = parse_sql_dependencies(new_sql)
        if op_type in ("INSERT", "UPDATE", "DELETE"):
            col_op = "WRITE"
    
    # Check table-level permissions for each referenced table.
    for (sch, tb, alias, is_sub) in parse_info.get("tables", []):
        if tb and not tb.startswith("(CTE)"):
            full_table = f"{sch}.{tb}" if sch else tb
            # For production, this check would query a permissions table.
            # Here, we assume permission check passes if not explicitly denied.
            logger.debug(f"Permission check for table {full_table} in group {rule_data['OWNER_GROUP']}.")
    
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        # Insert new rule.
        c.execute("""
            INSERT INTO BRM_RULES(
                GROUP_ID, PARENT_RULE_ID, RULE_TYPE_ID, RULE_NAME, RULE_SQL,
                EFFECTIVE_START_DATE, EFFECTIVE_END_DATE, STATUS, VERSION,
                CREATED_BY, DESCRIPTION, OPERATION_TYPE, BUSINESS_JUSTIFICATION,
                CREATED_TIMESTAMP, UPDATED_BY, OWNER_GROUP, CLUSTER_NAME,
                APPROVAL_STATUS, IS_GLOBAL, CRITICAL_RULE, CRITICAL_SCOPE, CDC_TYPE,
                LIFECYCLE_STATE, DECISION_TABLE_ID, ENCRYPTED_FLAG
            )
            OUTPUT inserted.RULE_ID
            VALUES(?,?,?,?,?,
                   ?,?, 'INACTIVE', 1,
                   ?,?,?,?,
                   ?, NULL, ?, ?,
                   'APPROVAL_IN_PROGRESS', ?, ?, ?, ?,
                   ?, ?, ?)
        """, (
            rule_data.get("GROUP_ID"),
            rule_data.get("PARENT_RULE_ID"),
            rule_data["RULE_TYPE_ID"],
            rule_data["RULE_NAME"].strip(),
            new_sql,
            rule_data["EFFECTIVE_START_DATE"],
            rule_data.get("EFFECTIVE_END_DATE"),
            created_by,
            rule_data.get("DESCRIPTION", ""),
            op_type,
            rule_data.get("BUSINESS_JUSTIFICATION", ""),
            now_str,
            rule_data["OWNER_GROUP"],
            rule_data.get("CLUSTER_NAME", ""),
            rule_data.get("IS_GLOBAL", 0),
            rule_data.get("CRITICAL_RULE", 0),
            rule_data.get("CRITICAL_SCOPE", "NONE"),
            rule_data.get("CDC_TYPE", "NONE"),
            rule_data["LIFECYCLE_STATE"],
            rule_data.get("DECISION_TABLE_ID"),
            rule_data.get("ENCRYPTED_FLAG", 0)
        ))
        new_rule_id = c.fetchone()[0]
        logger.info(f"New rule inserted with ID {new_rule_id}.")

        # Insert table dependencies.
        if op_type not in ("DECISION_TABLE", "OTHER") and new_sql:
            for (sch, tb, alias, is_sub) in parse_info.get("tables", []):
                if tb and not tb.startswith("(CTE)"):
                    c.execute("""
                        INSERT INTO BRM_RULE_TABLE_DEPENDENCIES(
                            RULE_ID, DATABASE_NAME, TABLE_NAME, COLUMN_NAME, COLUMN_OP
                        )
                        VALUES(?,?,?,?,?)
                    """, (new_rule_id, sch if sch else "N/A", tb, "AutoGenerated", col_op))
        # Log audit.
        insert_audit_log(conn, "INSERT", "BRM_RULES", new_rule_id, created_by, None, rule_data)
        conn.commit()
    except Exception as ex:
        conn.rollback()
        logger.error(f"Error inserting rule: {ex}")
        raise ex

    # Create multi-step approvals.
    create_multistep_approvals(conn, new_rule_id, created_by)
    return new_rule_id

def update_rule(conn, rule_data, updated_by, user_group):
    """
    Updates an existing rule.
    Re-validates table-level permissions, increments version,
    resets status to INACTIVE, and initiates a re‑approval flow.
    Also logs audit information.
    """
    c = conn.cursor()
    rule_id = rule_data["RULE_ID"]

    # Lock the rule for editing.
    lock_rule_for_edit(conn, rule_id, updated_by)
    try:
        c.execute("SELECT * FROM BRM_RULES WHERE RULE_ID=?", (rule_id,))
        old_row = c.fetchone()
        if not old_row:
            raise ValueError("Rule not found.")
        colnames = [d[0] for d in c.description]
        old_data = dict(zip(colnames, old_row))

        new_sql = rule_data.get("RULE_SQL", "").strip()
        op_type = detect_operation_type(new_sql, rule_data.get("DECISION_TABLE_ID"))
        rule_data["OPERATION_TYPE"] = op_type

        parse_info = {}
        col_op = "READ"
        if op_type not in ("DECISION_TABLE", "OTHER") and new_sql:
            parse_info = parse_sql_dependencies(new_sql)
            if op_type in ("INSERT", "UPDATE", "DELETE"):
                col_op = "WRITE"

        # Check table-level permissions.
        for (sch, tb, alias, is_sub) in parse_info.get("tables", []):
            if tb and not tb.startswith("(CTE)"):
                full_table = f"{sch}.{tb}" if sch else tb
                logger.debug(f"Re-validating permission for table {full_table} for group {old_data['OWNER_GROUP']}.")

        # Update the rule.
        c.execute("""
            UPDATE BRM_RULES
            SET GROUP_ID = ?,
                PARENT_RULE_ID = ?,
                RULE_TYPE_ID = ?,
                RULE_NAME = ?,
                RULE_SQL = ?,
                EFFECTIVE_START_DATE = ?,
                EFFECTIVE_END_DATE = ?,
                STATUS = 'INACTIVE',
                VERSION = VERSION + 1,
                UPDATED_BY = ?,
                DESCRIPTION = ?,
                OPERATION_TYPE = ?,
                BUSINESS_JUSTIFICATION = ?,
                OWNER_GROUP = ?,
                CLUSTER_NAME = ?,
                APPROVAL_STATUS = 'APPROVAL_IN_PROGRESS',
                IS_GLOBAL = ?,
                CRITICAL_RULE = ?,
                CRITICAL_SCOPE = ?,
                CDC_TYPE = ?,
                LIFECYCLE_STATE = 'UNDER_APPROVAL',
                DECISION_TABLE_ID = ?,
                ENCRYPTED_FLAG = ?
            WHERE RULE_ID = ?
        """, (
            rule_data.get("GROUP_ID", old_data["GROUP_ID"]),
            rule_data.get("PARENT_RULE_ID", old_data.get("PARENT_RULE_ID")),
            rule_data["RULE_TYPE_ID"],
            rule_data["RULE_NAME"].strip(),
            new_sql,
            rule_data["EFFECTIVE_START_DATE"],
            rule_data.get("EFFECTIVE_END_DATE"),
            updated_by,
            rule_data.get("DESCRIPTION", old_data.get("DESCRIPTION", "")),
            op_type,
            rule_data.get("BUSINESS_JUSTIFICATION", old_data.get("BUSINESS_JUSTIFICATION", "")),
            rule_data.get("OWNER_GROUP", old_data.get("OWNER_GROUP")),
            rule_data.get("CLUSTER_NAME", old_data.get("CLUSTER_NAME", "")),
            rule_data.get("IS_GLOBAL", old_data.get("IS_GLOBAL", 0)),
            rule_data.get("CRITICAL_RULE", old_data.get("CRITICAL_RULE", 0)),
            rule_data.get("CRITICAL_SCOPE", old_data.get("CRITICAL_SCOPE", "NONE")),
            rule_data.get("CDC_TYPE", old_data.get("CDC_TYPE", "NONE")),
            rule_data.get("DECISION_TABLE_ID"),
            rule_data.get("ENCRYPTED_FLAG", old_data.get("ENCRYPTED_FLAG", 0)),
            rule_id
        ))
        # Delete old dependencies and re-insert new ones.
        c.execute("DELETE FROM BRM_RULE_TABLE_DEPENDENCIES WHERE RULE_ID=?", (rule_id,))
        if op_type not in ("DECISION_TABLE", "OTHER") and new_sql:
            for (sch, tb, alias, is_sub) in parse_info.get("tables", []):
                if tb and not tb.startswith("(CTE)"):
                    c.execute("""
                        INSERT INTO BRM_RULE_TABLE_DEPENDENCIES(
                            RULE_ID, DATABASE_NAME, TABLE_NAME, COLUMN_NAME, COLUMN_OP
                        )
                        VALUES(?,?,?,?,?)
                    """, (rule_id, sch if sch else "N/A", tb, "ReParsed", col_op))
        new_data = old_data.copy()
        new_data.update(rule_data)
        new_data["VERSION"] = old_data["VERSION"] + 1

        insert_audit_log(conn, "UPDATE", "BRM_RULES", rule_id, updated_by, old_data, new_data)
        conn.commit()
    except Exception as ex:
        conn.rollback()
        logger.error(f"Error updating rule {rule_id}: {ex}")
        raise ex
    finally:
        unlock_rule(conn, rule_id, updated_by)

def deactivate_rule(conn, rule_id, updated_by, user_group, force=False):
    """
    Deactivates a rule (sets status to INACTIVE).
    Checks that the rule is fully approved unless forced.
    Also ensures no active child rules exist unless force is used.
    """
    lock_rule_for_edit(conn, rule_id, updated_by, force=force)
    try:
        c = conn.cursor()
        c.execute("SELECT * FROM BRM_RULES WHERE RULE_ID=?", (rule_id,))
        old_row = c.fetchone()
        if not old_row:
            raise ValueError("Rule not found.")
        colnames = [d[0] for d in c.description]
        old_data = dict(zip(colnames, old_row))
        if old_data["APPROVAL_STATUS"] != "APPROVED" and not force:
            raise ValueError("Rule is not fully approved; cannot deactivate without force.")
        # Check for active child rules.
        c.execute("SELECT 1 FROM BRM_RULES WHERE PARENT_RULE_ID=? AND STATUS='ACTIVE'", (rule_id,))
        if c.fetchone() and not force:
            raise ValueError("Active child rules exist; deactivate them first or use force.")
        c.execute("""
            UPDATE BRM_RULES
            SET STATUS = 'INACTIVE',
                LIFECYCLE_STATE = 'INACTIVE',
                UPDATED_BY = ?,
                VERSION = VERSION + 1,
                APPROVAL_STATUS = 'DEACTIVATED'
            WHERE RULE_ID = ?
        """, (updated_by, rule_id))
        new_data = old_data.copy()
        new_data["STATUS"] = "INACTIVE"
        new_data["LIFECYCLE_STATE"] = "INACTIVE"
        new_data["VERSION"] = old_data["VERSION"] + 1
        insert_audit_log(conn, "DEACTIVATE", "BRM_RULES", rule_id, updated_by, old_data, new_data)
        conn.commit()
    except Exception as ex:
        conn.rollback()
        logger.error(f"Error deactivating rule {rule_id}: {ex}")
        raise ex
    finally:
        unlock_rule(conn, rule_id, updated_by, force=force)

def delete_rule(conn, rule_id, action_by, user_group, force=False):
    """
    Deletes a rule from BRM_RULES.
    Requires the rule to be inactive and fully approved unless force is used.
    Also checks that no child rules or column mapping references exist.
    """
    lock_rule_for_edit(conn, rule_id, action_by, force=force)
    try:
        c = conn.cursor()
        c.execute("SELECT * FROM BRM_RULES WHERE RULE_ID=?", (rule_id,))
        old_row = c.fetchone()
        if not old_row:
            raise ValueError("Rule not found.")
        colnames = [d[0] for d in c.description]
        old_data = dict(zip(colnames, old_row))
        if old_data["IS_GLOBAL"] == 1 and user_group != "Admin":
            raise ValueError("Only Admin can delete global rules.")
        if old_data["APPROVAL_STATUS"] != "APPROVED" and not force:
            raise ValueError("Rule not fully approved; cannot delete without force.")
        if old_data["STATUS"] != "INACTIVE" and not force:
            raise ValueError("Rule must be INACTIVE before deletion (or use force).")
        c.execute("SELECT 1 FROM BRM_RULES WHERE PARENT_RULE_ID=?", (rule_id,))
        if c.fetchone():
            raise ValueError("Rule has child rules; delete or reassign them first.")
        # Additional reference checks can be added here.
        c.execute("DELETE FROM BRM_RULES WHERE RULE_ID=?", (rule_id,))
        insert_audit_log(conn, "DELETE", "BRM_RULES", rule_id, action_by, old_data, None)
        conn.commit()
    except Exception as ex:
        conn.rollback()
        logger.error(f"Error deleting rule {rule_id}: {ex}")
        raise ex
    finally:
        unlock_rule(conn, rule_id, action_by, force=force)

# =============================================================================
# MULTI-STEP APPROVALS & RELATED FUNCTIONS
# =============================================================================
def create_multistep_approvals(conn, rule_id, initiated_by):
    """
    Creates approval entries in BRM_RULE_APPROVALS for the given rule.
    The approval pipeline is determined based on impacted business groups,
    table references, and decision table usage.
    """
    impacted_groups = find_impacted_business_groups(conn, rule_id)
    # For simplicity, build a pipeline: BG1 always, then any additional groups, then FINAL.
    pipeline = ["BG1"] + list(impacted_groups - {"BG1"}) + ["FINAL"]
    c = conn.cursor()
    c.execute("DELETE FROM BRM_RULE_APPROVALS WHERE RULE_ID=?", (rule_id,))
    stage_ctr = 1
    for grp in pipeline:
        # For FINAL stage, assign a default approver.
        if grp == "FINAL":
            c.execute("""
                INSERT INTO BRM_RULE_APPROVALS(
                    RULE_ID, GROUP_NAME, USERNAME, APPROVED_FLAG,
                    APPROVAL_TIMESTAMP, APPROVAL_STAGE
                )
                VALUES(?, ?, ?, 0, NULL, ?)
            """, (rule_id, "FINAL", "final_approver", stage_ctr))
        else:
            # Retrieve approvers for the group.
            c.execute("SELECT USERNAME FROM BUSINESS_GROUP_APPROVERS WHERE GROUP_NAME=?", (grp,))
            approvers = c.fetchall()
            if not approvers:
                # If no approvers found, insert a placeholder.
                c.execute("""
                    INSERT INTO BRM_RULE_APPROVALS(
                        RULE_ID, GROUP_NAME, USERNAME, APPROVED_FLAG,
                        APPROVAL_TIMESTAMP, APPROVAL_STAGE
                    )
                    VALUES(?, ?, ?, 0, NULL, ?)
                """, (rule_id, grp, f"{grp}_approver", stage_ctr))
            else:
                for (appr_user,) in approvers:
                    c.execute("""
                        INSERT INTO BRM_RULE_APPROVALS(
                            RULE_ID, GROUP_NAME, USERNAME, APPROVED_FLAG,
                            APPROVAL_TIMESTAMP, APPROVAL_STAGE
                        )
                        VALUES(?, ?, ?, 0, NULL, ?)
                    """, (rule_id, grp, appr_user, stage_ctr))
        stage_ctr += 1
    conn.commit()
    logger.info(f"Multi-step approvals created for rule {rule_id} with pipeline: {pipeline}")

def find_impacted_business_groups(conn, rule_id):
    """
    Uses BFS to traverse related rules (via parent-child and global-critical links)
    and returns a set of impacted business groups.
    """
    visited = unified_get_related_rules(conn, rule_id)
    groups = set()
    c = conn.cursor()
    for rid in visited:
        c.execute("SELECT OWNER_GROUP FROM BRM_RULES WHERE RULE_ID=?", (rid,))
        row = c.fetchone()
        if row:
            groups.add(row[0])
    return groups

def unified_get_related_rules(conn, start_rule_id):
    """
    Returns a set of all rule IDs related to the given rule ID using BFS.
    Traverses parent-child relationships and global-critical links.
    """
    adjacency, _, _ = load_rule_relationships(conn)
    visited = set()
    queue = [start_rule_id]
    while queue:
        cur = queue.pop(0)
        if cur in visited:
            continue
        visited.add(cur)
        if cur in adjacency:
            for neighbor in adjacency[cur]:
                if neighbor not in visited:
                    queue.append(neighbor)
    return visited

# =============================================================================
# LOCK/UNLOCK FUNCTIONS FOR RULE EDITING
# =============================================================================
def lock_rule_for_edit(conn, rule_id, locked_by, force=False):
    """
    Attempts to lock a rule for editing.
    If already locked by another user, raises an error unless force is True.
    Also auto-unlocks locks older than 30 minutes.
    """
    c = conn.cursor()
    try:
        c.execute("""
            DELETE FROM BRM_RULE_LOCKS
            WHERE DATEDIFF(MINUTE, LOCK_TIMESTAMP, GETDATE()) > 30
        """)
        conn.commit()
    except Exception as ex:
        logger.error(f"Auto-unlock error: {ex}")
    c.execute("SELECT LOCKED_BY FROM BRM_RULE_LOCKS WHERE RULE_ID=?", (rule_id,))
    row = c.fetchone()
    if row:
        existing = row[0]
        if existing != locked_by and not force:
            raise ValueError(f"Rule {rule_id} is locked by {existing}.")
        else:
            c.execute("DELETE FROM BRM_RULE_LOCKS WHERE RULE_ID=?", (rule_id,))
    c.execute("""
        INSERT INTO BRM_RULE_LOCKS (RULE_ID, LOCKED_BY, LOCK_TIMESTAMP)
        VALUES (?, ?, GETDATE())
    """, (rule_id, locked_by))
    conn.commit()
    logger.info(f"Rule {rule_id} locked by {locked_by}.")

def unlock_rule(conn, rule_id, locked_by, force=False):
    """
    Unlocks the rule if the current user owns the lock or if force is True.
    """
    c = conn.cursor()
    c.execute("SELECT LOCKED_BY FROM BRM_RULE_LOCKS WHERE RULE_ID=?", (rule_id,))
    row = c.fetchone()
    if row:
        existing = row[0]
        if existing != locked_by and not force:
            raise ValueError(f"Cannot unlock rule {rule_id}; locked by {existing}.")
        c.execute("DELETE FROM BRM_RULE_LOCKS WHERE RULE_ID=?", (rule_id,))
        conn.commit()
        logger.info(f"Rule {rule_id} unlocked by {locked_by}.")

# =============================================================================
# PERFORMANCE & SIMULATION LOGGING FUNCTIONS
# =============================================================================
def insert_rule_execution_log(conn, rule_id, pass_flag, message, record_count):
    """
    Inserts an entry into RULE_EXECUTION_LOGS capturing execution results.
    Also logs the execution time and record count.
    """
    c = conn.cursor()
    try:
        c.execute("""
            INSERT INTO RULE_EXECUTION_LOGS(
                RULE_ID, EXECUTION_TIMESTAMP, PASS_FLAG, MESSAGE, RECORD_COUNT,
                EXECUTION_TIME_MS, CPU_USAGE, MEM_USAGE
            )
            VALUES(?, GETDATE(), ?, ?, ?, ?, 0, 0)
        """, (rule_id, 1 if pass_flag else 0, message, record_count, 0))
        conn.commit()
        logger.info(f"Execution log inserted for rule {rule_id}.")
    except Exception as ex:
        conn.rollback()
        logger.error(f"Error inserting execution log for rule {rule_id}: {ex}")

# =============================================================================
# LOAD RULE RELATIONSHIPS (BFS helpers)
# =============================================================================
def load_rule_relationships(conn):
    """
    Constructs adjacency relationships from BRM_RULES and BRM_GLOBAL_CRITICAL_LINKS.
    Returns (adjacency, roots, parent_map).
    """
    c = conn.cursor()
    c.execute("SELECT RULE_ID, PARENT_RULE_ID FROM BRM_RULES")
    rows = c.fetchall()
    adjacency = {}
    parent_map = {}
    all_ids = set()
    for (rid, pid) in rows:
        all_ids.add(rid)
        if pid:
            adjacency.setdefault(pid, set()).add(rid)
            parent_map[rid] = pid
    # Global-critical links
    c.execute("SELECT GCR_RULE_ID, TARGET_RULE_ID FROM BRM_GLOBAL_CRITICAL_LINKS")
    for (gcr, tgt) in c.fetchall():
        adjacency.setdefault(gcr, set()).add(tgt)
    # Conflicts (bidirectional)
    c.execute("SELECT RULE_ID1, RULE_ID2 FROM RULE_CONFLICTS")
    for (r1, r2) in c.fetchall():
        adjacency.setdefault(r1, set()).add(r2)
        adjacency.setdefault(r2, set()).add(r1)
    roots = [r for r in all_ids if r not in parent_map]
    return (adjacency, roots, parent_map)

def skip_all_descendants(start_id, adjacency, skipped):
    """
    Recursively marks all descendant rules (via BFS) as skipped.
    """
    queue = [start_id]
    while queue:
        cur = queue.pop()
        if cur in skipped:
            continue
        skipped.add(cur)
        if cur in adjacency:
            for child in adjacency[cur]:
                if child not in skipped:
                    queue.append(child)

# =============================================================================
# EXPORTS
# =============================================================================
__all__ = [
    "fetch_all_dict", "fetch_one_dict", "insert_audit_log",
    "detect_operation_type", "parse_sql_dependencies",
    "run_single_rule_transaction", "advanced_bfs_execute", "dry_run_bfs",
    "execute_rule_simulation", "add_rule", "update_rule", "deactivate_rule", "delete_rule",
    "create_multistep_approvals", "find_impacted_business_groups", "unified_get_related_rules",
    "lock_rule_for_edit", "unlock_rule", "insert_rule_execution_log",
    "load_rule_relationships", "skip_all_descendants"
]

if __name__ == "__main__":
    # For testing purposes only
    try:
        # Sample connection string – replace with valid DSN or connection string for testing
        conn = pyodbc.connect("DSN=YourDSN;Trusted_Connection=yes;")
        logger.info("Connected to DB for testing.")
    except Exception as ex:
        logger.error(f"DB connection failed: {ex}")
        sys.exit(1)

    # Run a test dry-run simulation on a sample rule ID (adjust as needed)
    test_rule_id = 1  # Adjust as appropriate for testing
    success, sim_msg, rec_count = execute_rule_simulation(conn, test_rule_id)
    print(f"Simulation result for Rule {test_rule_id}: {'PASS' if success else 'FAIL'}; {sim_msg}; Impacted {rec_count} record(s).")
