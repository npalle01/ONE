#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Module: brm_tool_db_helpers.py
Description:
  - Provides production‑ready database helper functions including:
    • fetch_all_dict and fetch_one_dict for result set conversion.
    • insert_audit_log to record changes (with JSON‑encoded data).
    • Advanced SQL parser to extract table, column, and CTE references.
    • Concurrency lock/unlock functions (lock_rule and unlock_rule) to prevent concurrent rule editing.
    • Simulation routines (run_dry_run_rule and run_single_rule_simulation) that execute rule SQL in dry‑run mode,
      capturing performance metrics and the number of impacted records.
  - All functions include robust error handling and logging.
"""

import logging
import json
import re
import sqlparse
from sqlparse.sql import Identifier, IdentifierList, Parenthesis
from sqlparse.tokens import Keyword, DML

# Configure module logging
logger = logging.getLogger("brm_tool_db_helpers")
logger.setLevel(logging.DEBUG)

# ---------------------------------------------------------------------------
# Basic DB Helpers
# ---------------------------------------------------------------------------
def fetch_all_dict(cursor):
    """
    Fetch all rows from the given cursor and return as a list of dictionaries.
    """
    rows = cursor.fetchall()
    if cursor.description:
        cols = [desc[0] for desc in cursor.description]
        return [dict(zip(cols, row)) for row in rows]
    return rows

def fetch_one_dict(cursor):
    """
    Fetch a single row from the given cursor and return it as a dictionary.
    """
    row = cursor.fetchone()
    if row and cursor.description:
        cols = [desc[0] for desc in cursor.description]
        return dict(zip(cols, row))
    return None

def insert_audit_log(conn, action, table_name, record_id, actor, old_data, new_data):
    """
    Insert an audit log entry into BRM_AUDIT_LOG.
    The old and new data are stored as JSON strings.
    """
    try:
        c = conn.cursor()
        c.execute("""
            INSERT INTO BRM_AUDIT_LOG (
                ACTION, TABLE_NAME, RECORD_ID, ACTION_BY,
                OLD_DATA, NEW_DATA, ACTION_TIMESTAMP
            )
            VALUES (?,?,?,?,?,? , GETDATE())
        """, (
            action,
            table_name,
            str(record_id) if record_id else None,
            actor,
            json.dumps(old_data) if old_data else None,
            json.dumps(new_data) if new_data else None
        ))
        conn.commit()
        logger.info(f"Audit log inserted for {action} on {table_name} (Record ID: {record_id}).")
    except Exception as ex:
        logger.error(f"Failed to insert audit log for {table_name} (Record ID: {record_id}): {ex}")
        raise

# ---------------------------------------------------------------------------
# Advanced SQL Parser
# ---------------------------------------------------------------------------
def parse_sql_dependencies(sql_text: str):
    """
    Parse the given SQL text using sqlparse and return a dictionary containing:
      - 'tables': list of (schema, table, alias, is_subquery) tuples,
      - 'cte_tables': list of (cte_name, references) tuples,
      - 'alias_map': dictionary mapping alias to (schema, table),
      - 'columns': list of (column_name, is_write, is_read) tuples.
    """
    statements = sqlparse.parse(sql_text)
    all_tables = []
    cte_info = []
    alias_map = {}
    columns = []

    for stmt in statements:
        ctes = _extract_with_clauses(stmt)
        for cte_name, cte_refs in ctes.items():
            cte_info.append((cte_name, cte_refs))
        main_refs, main_alias = _extract_main_from(stmt.tokens, set(ctes.keys()))
        all_tables.extend(main_refs)
        alias_map.update(main_alias)
        col_refs = _extract_columns(stmt)
        columns.extend(col_refs)

    unique_tables = list({t for t in all_tables if t[1]})
    return {
        "tables": unique_tables,
        "cte_tables": cte_info,
        "alias_map": alias_map,
        "columns": columns
    }

# --- Helper functions for the SQL parser ---
def _extract_with_clauses(statement):
    cte_map = {}
    tokens = list(statement.tokens)
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if token.ttype is Keyword and token.value.upper() == "WITH":
            i += 1
            i = _parse_cte_block(tokens, i, cte_map)
            continue
        i += 1
    return cte_map

def _parse_cte_block(tokens, i, cte_map):
    while i < len(tokens):
        token = tokens[i]
        if isinstance(token, Identifier):
            cte_name = token.get_real_name()
            i += 1
            i = _parse_cte_as_clause(tokens, i, cte_name, cte_map)
        elif token.ttype is Keyword and token.value.upper() in ("SELECT", "INSERT", "UPDATE", "DELETE"):
            return i
        else:
            i += 1
    return i

def _parse_cte_as_clause(tokens, i, cte_name, cte_map):
    while i < len(tokens):
        token = tokens[i]
        if token.value.upper() == "AS":
            i += 1
            if i < len(tokens):
                sub = tokens[i]
                if isinstance(sub, Parenthesis):
                    sub_refs = _extract_subselect_tokens(sub.tokens)
                    cte_map[cte_name] = sub_refs
                    i += 1
                    return i
        else:
            i += 1
    return i

def _extract_subselect_tokens(tokens):
    results = []
    from_seen = False
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if token.is_group and _is_subselect(token):
            results.extend(_extract_subselect_tokens(token.tokens))
        if token.ttype is Keyword:
            upv = token.value.upper()
            if upv in ("FROM", "JOIN", "INNER JOIN", "LEFT JOIN", "RIGHT JOIN", "FULL JOIN"):
                from_seen = True
            else:
                from_seen = False
        if from_seen:
            if isinstance(token, IdentifierList):
                for ident in token.get_identifiers():
                    parsed = _parse_identifier(ident, set())
                    results.append((parsed[0], parsed[1], parsed[2], True))
            elif isinstance(token, Identifier):
                parsed = _parse_identifier(token, set())
                results.append((parsed[0], parsed[1], parsed[2], True))
        i += 1
    return results

def _is_subselect(token):
    if not token.is_group:
        return False
    for sub in token.tokens:
        if sub.ttype is DML and sub.value.upper() == "SELECT":
            return True
    return False

def _extract_main_from(tokenlist, known_cte_names):
    results = []
    alias_map = {}
    tokens = list(tokenlist)
    from_seen = False
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if token.is_group and _is_subselect(token):
            results.extend(_extract_subselect_tokens(token.tokens))
        if token.ttype is Keyword:
            upv = token.value.upper()
            if upv in ("FROM", "JOIN", "INNER JOIN", "LEFT JOIN", "RIGHT JOIN", "FULL JOIN"):
                from_seen = True
            else:
                from_seen = False
        if from_seen:
            if isinstance(token, IdentifierList):
                for ident in token.get_identifiers():
                    parsed = _parse_identifier(ident, known_cte_names)
                    results.append(parsed)
                    if parsed[2]:
                        alias_map[parsed[2]] = (parsed[0], parsed[1])
            elif isinstance(token, Identifier):
                parsed = _parse_identifier(token, known_cte_names)
                results.append(parsed)
                if parsed[2]:
                    alias_map[parsed[2]] = (parsed[0], parsed[1])
        i += 1
    return (results, alias_map)

def _parse_identifier(ident, known_cte_names):
    alias = ident.get_alias()
    real_name = ident.get_real_name()
    schema_name = ident.get_parent_name()
    if real_name and real_name.upper() in (n.upper() for n in known_cte_names):
        return (None, f"(CTE) {real_name}", alias, False)
    return (schema_name, real_name, alias, False)

def _extract_columns(statement):
    results = []
    tokens = list(statement.tokens)
    i = 0
    while i < len(tokens):
        token = tokens[i]
        if token.ttype is DML:
            word = token.value.upper()
            if word == "SELECT":
                cols = _parse_select_list(tokens, i + 1)
                for col in cols:
                    results.append((col, False, True))
            elif word in ("INSERT", "UPDATE"):
                cols = _parse_dml_columns(tokens, i, word)
                for col in cols:
                    results.append((col, True, False))
        i += 1
    return results

def _parse_select_list(tokens, start_idx):
    columns = []
    i = start_idx
    while i < len(tokens):
        token = tokens[i]
        if token.ttype is Keyword and token.value.upper() in ("FROM", "JOIN", "WHERE", "GROUP", "ORDER", "UNION", "INTERSECT"):
            break
        if isinstance(token, IdentifierList):
            for ident in token.get_identifiers():
                nm = ident.get_name()
                if nm and nm.upper() not in ("DISTINCT", "TOP", "ALL"):
                    columns.append(nm)
        elif isinstance(token, Identifier):
            nm = token.get_name()
            if nm and nm.upper() not in ("DISTINCT", "TOP", "ALL"):
                columns.append(nm)
        i += 1
    return columns

def _parse_dml_columns(tokens, start_idx, dml_word):
    columns = []
    if dml_word == "INSERT":
        i = start_idx
        while i < len(tokens):
            token = tokens[i]
            if token.is_group and isinstance(token, Parenthesis):
                for subtoken in token.tokens:
                    if isinstance(subtoken, IdentifierList):
                        for ident in subtoken.get_identifiers():
                            columns.append(ident.get_name())
                    elif isinstance(subtoken, Identifier):
                        columns.append(subtoken.get_name())
                return columns
            i += 1
    elif dml_word == "UPDATE":
        i = start_idx
        while i < len(tokens):
            token = tokens[i]
            if token.ttype is Keyword and token.value.upper() == "SET":
                i += 1
                columns.extend(_parse_update_set_list(tokens, i))
                break
            i += 1
    return columns

def _parse_update_set_list(tokens, start_i):
    columns = []
    i = start_i
    while i < len(tokens):
        token = tokens[i]
        if token.ttype is Keyword and token.value.upper() in ("WHERE", "FROM"):
            break
        if isinstance(token, Identifier):
            columns.append(token.get_name())
        i += 1
    return columns

# ---------------------------------------------------------------------------
# Concurrency Locking Functions
# ---------------------------------------------------------------------------
def lock_rule(conn, rule_id, user_id, user_group, lock_minutes=30):
    """
    Acquire a lock on the specified rule by inserting into the RULE_LOCKS table.
    If the rule is already locked by another user and the current user is not admin (or force flag not set),
    raise an error. Auto‑cleanup locks older than lock_minutes.
    """
    c = conn.cursor()
    try:
        # Clean up stale locks (older than lock_minutes)
        c.execute("""
            DELETE FROM BRM_RULE_LOCKS
            WHERE DATEDIFF(MINUTE, LOCK_TIMESTAMP, GETDATE()) > ?
        """, (lock_minutes,))
        conn.commit()

        c.execute("""
            SELECT LOCKED_BY FROM BRM_RULE_LOCKS WHERE RULE_ID=?
        """, (rule_id,))
        row = c.fetchone()
        if row:
            existing_lock = row[0]
            if existing_lock != user_id and user_group != "Admin":
                raise ValueError(f"Rule {rule_id} is currently locked by {existing_lock}.")
            else:
                # If admin or same user, override lock
                c.execute("DELETE FROM BRM_RULE_LOCKS WHERE RULE_ID=?", (rule_id,))
        c.execute("""
            INSERT INTO BRM_RULE_LOCKS (RULE_ID, LOCKED_BY, LOCK_TIMESTAMP)
            VALUES (?, ?, GETDATE())
        """, (rule_id, user_id))
        conn.commit()
        logger.info(f"Rule {rule_id} locked by {user_id} for {lock_minutes} minutes.")
    except Exception as ex:
        logger.error(f"Error locking rule {rule_id}: {ex}")
        raise

def unlock_rule(conn, rule_id, user_id, user_group):
    """
    Release the lock on the specified rule if it is held by the user or if the user is an admin.
    """
    c = conn.cursor()
    c.execute("""
        SELECT LOCKED_BY FROM BRM_RULE_LOCKS WHERE RULE_ID=?
    """, (rule_id,))
    row = c.fetchone()
    if row:
        locked_by = row[0]
        if locked_by != user_id and user_group != "Admin":
            raise ValueError(f"You do not have permission to unlock rule {rule_id} locked by {locked_by}.")
        c.execute("DELETE FROM BRM_RULE_LOCKS WHERE RULE_ID=?", (rule_id,))
        conn.commit()
        logger.info(f"Rule {rule_id} unlocked by {user_id}.")
    else:
        logger.info(f"No active lock for rule {rule_id}.")

# ---------------------------------------------------------------------------
# Simulation Functions for Dry-Run Execution Logging
# ---------------------------------------------------------------------------
def run_dry_run_rule(conn, rule_info):
    """
    Execute the rule SQL in dry-run mode (always roll back) and capture:
      - pass_flag (True if the SQL returns a row whose first value equals 1),
      - a message describing the result,
      - record_count: the number of rows returned.
    Also logs the simulation result.
    """
    c = conn.cursor()
    c.execute("BEGIN TRANSACTION")
    success = False
    msg = ""
    record_count = 0
    try:
        c.execute(rule_info.get("RULE_SQL", ""))
        rows = c.fetchall()
        record_count = len(rows)
        if rows:
            first_val = rows[0][0]
            success = (first_val == 1)
            msg = f"Returned: {first_val} from {record_count} row(s)"
        else:
            success = True
            msg = "No rows returned; interpreted as PASS"
        # Always roll back in dry-run mode
        c.execute("ROLLBACK")
        logger.info(f"Dry-run for Rule {rule_info.get('RULE_ID')} => {msg}")
    except Exception as ex:
        c.execute("ROLLBACK")
        success = False
        msg = f"Error during dry-run: {ex}"
        logger.error(f"Dry-run error for Rule {rule_info.get('RULE_ID')}: {ex}")
    return success, msg, record_count

def run_single_rule_simulation(conn, rule_info, is_dry_run=True):
    """
    Execute a single rule transaction.
    If is_dry_run is True, the transaction is always rolled back.
    Captures performance metrics (e.g. elapsed time) and returns a tuple:
      (success_flag, message, record_count, elapsed_ms)
    """
    import time
    c = conn.cursor()
    c.execute("BEGIN TRANSACTION")
    start_time = time.time()
    success = False
    msg = ""
    record_count = 0
    try:
        c.execute(rule_info.get("RULE_SQL", ""))
        rows = c.fetchall()
        record_count = len(rows)
        if rows:
            first_val = rows[0][0]
            success = (first_val == 1)
            msg = f"Returned: {first_val} from {record_count} row(s)"
        else:
            success = True
            msg = "No rows returned; interpreted as PASS"
        if is_dry_run:
            c.execute("ROLLBACK")
        else:
            if success:
                c.execute("COMMIT")
            else:
                c.execute("ROLLBACK")
        elapsed_ms = (time.time() - start_time) * 1000.0
        logger.info(f"Rule {rule_info.get('RULE_ID')} simulation: {msg} (Elapsed: {elapsed_ms:.2f}ms)")
    except Exception as ex:
        c.execute("ROLLBACK")
        elapsed_ms = (time.time() - start_time) * 1000.0
        success = False
        msg = f"Exception during simulation: {ex}"
        logger.error(f"Simulation error for Rule {rule_info.get('RULE_ID')}: {ex}")
    return success, msg, record_count, elapsed_ms

# For backward compatibility: you may call run_dry_run_rule() directly

# ---------------------------------------------------------------------------
# Example function to log simulation results into RULE_EXECUTION_LOGS
# ---------------------------------------------------------------------------
def insert_rule_execution_log(conn, rule_id, pass_flag, message, record_count, execution_time_ms=0):
    """
    Insert an execution log for a rule into RULE_EXECUTION_LOGS.
    Records the rule ID, timestamp, pass/fail flag, message, record count, and execution time.
    """
    try:
        c = conn.cursor()
        c.execute("""
            INSERT INTO RULE_EXECUTION_LOGS (
                RULE_ID, EXECUTION_TIMESTAMP, PASS_FLAG, MESSAGE, RECORD_COUNT, EXECUTION_TIME_MS
            )
            VALUES (?, GETDATE(), ?, ?, ?, ?)
        """, (rule_id, 1 if pass_flag else 0, message, record_count, execution_time_ms))
        conn.commit()
        logger.info(f"Execution log inserted for Rule {rule_id} (Pass: {pass_flag}, Records: {record_count}).")
    except Exception as ex:
        logger.error(f"Error inserting execution log for Rule {rule_id}: {ex}")
        raise

# ---------------------------------------------------------------------------
# End of Module: brm_tool_db_helpers.py
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    # For testing purposes, simulate a dry-run execution for a dummy rule.
    # (In production, this module is imported by other parts of the application.)
    class DummyCursor:
        def __init__(self):
            self.description = [("col1",)]
        def execute(self, query, params=None):
            pass
        def fetchall(self):
            return [(1,)]
        def fetchone(self):
            return (1,)
    class DummyConn:
        def cursor(self):
            return DummyCursor()
        def commit(self):
            pass
    dummy_conn = DummyConn()
    dummy_rule = {"RULE_ID": 101, "RULE_SQL": "SELECT 1"}
    result = run_dry_run_rule(dummy_conn, dummy_rule)
    print("Dry-run simulation result:", result)
    