#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
BRM MASTER CODE (Consolidated + Enhanced)
Part 1/?

This integrated code merges:
 • Original foundation (Parts 1–8)
 • Advanced Search & Filtering (multi-criteria, saved filters, suggestions)
 • Intelligent Rule Editing & SQL Assistance (SQL validation, warnings, templates)
 • Enhanced Onboarding & In-App Guidance
 • Threaded Collaboration & Mentions
 • Extended Hierarchy & Lineage Graph Interactions
 • "What-If" Testing with interactive test data editor
 • Multi-step approvals with richer pipeline visualization & parallel options
 • Task & Notification Center
 • Richer Conflict Resolution Tools (visual map)
 • Enhanced Snapshots & Version Diff
 • Usage Analytics & Heatmaps
 • AIOps (Anomaly detection & auto-escalation)
 • Configurable Roles & Permissions (RBAC/ABAC approach)
 • Visually Rich Approval Pipeline
Plus the entire original code for DB connection, logging, BFS execution, synchronization, etc.

NOTE:
 - This is a “best-effort” integrated code. It may not match your original line numbering or exact text from the prior 6000+ lines. 
 - It includes placeholders or partial logic for advanced features (like advanced RBAC) that would require additional DB schema & logic.

Contents:
  (1) Imports & Logging Config
  (2) DB Connection Dialog
  (3) Basic & Advanced DB Helpers
  (4) EmailNotifier & Collaboration
  (5) LockManager (unified approach)
  (6) Advanced SQL Parser
  (7) Login Dialog
  (8) Onboarding Wizard
  (9) sync_metadata_improved
  (10) BFS Rule Execution & Data Validations
  (11) Decision Table Execution
  (12) Dry-run Simulations
  (13) GUI Modules:
      • EnhancedScheduleDialog, ScheduleManagementTab
      • ChainSimulationDialog, GroupSimulationDialog
      • DecisionTablesTab, ConflictPriorityManagerTab
      • CompositeRulesTab, SnapshotManagerTab, TagsManagerTab
      • DataValidationTab, WhatIfTestTab
  (14) Collaboration, Custom Groups, Extended Hierarchy
  (15) MultiStepApprovalTab (extended pipeline UI)
  (16) GlobalCriticalAdminTab
  (17) EnhancedLineageGraphWidget
  (18) MetricsDashboardTab with Heatmaps
  (19) CtrlTablesTab, GroupManagementTab, UserManagementTab
  (20) Advanced RBAC/ABAC placeholders
  (21) BusinessRulesTab (with advanced search & filtering)
  (22) ApprovalsTab (with comments & partial or parallel approvals)
  (23) SchedulingTab
  (24) DefectManagementTab
  (25) CollaborationTab (threaded + mention)
  (26) AuditLogViewer with advanced search & CSV/JSON/Excel export
  (27) RuleSearchDialog
  (28) VersionHistoryDialog with diff
  (29) RuleDashboard
  (30) RuleEditorDialog (with SQL assistance, template library)
  (31) Additional advanced features (Richer conflict resolution map, anomaly detection, auto-healing, etc.)
  (32) MainWindow Integration + Task/Notification Center
  (33) HelpFeedbackTab, advanced Onboarding integration
  (34) main() function with everything tied together
"""

import sys
import os
import json
import math
import csv
import re
import logging
import logging.config
import difflib
import random
from datetime import datetime, timedelta
from collections import defaultdict, deque
from sklearn.linear_model import LinearRegression  # for simple anomaly/trend detection

# PyQt5 imports
from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtCore import Qt, QDateTime, QTimer, QDate, QTime, QPointF, pyqtSignal
from PyQt5.QtGui import QFont, QColor, QPen, QBrush, QIcon
from PyQt5.QtWidgets import (
    QDialog, QVBoxLayout, QLabel, QLineEdit, QPushButton, QHBoxLayout, 
    QMessageBox, QComboBox, QPlainTextEdit, QCalendarWidget, QTimeEdit, 
    QFormLayout, QWidget, QCheckBox, QTableWidget, QTableWidgetItem,
    QInputDialog, QMenu, QListWidget, QListWidgetItem, QTreeWidget,
    QTreeWidgetItem, QDockWidget, QMainWindow, QTabWidget, QGroupBox,
    QSpinBox, QProgressDialog, QSplitter, QFileDialog
)

import smtplib
from email.mime.text import MIMEText

import pyodbc
import sqlparse
import pyqtgraph as pg

##############################################################################
# (1) LOGGING CONFIG & GLOBAL SETTINGS
##############################################################################

# Enhanced Logging Configuration (can be moved to a separate file if needed)
LOG_CONFIG = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "standard": {
            "format": "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
        }
    },
    "handlers": {
        "console": {
            "class": "logging.StreamHandler",
            "formatter": "standard",
            "level": "DEBUG"
        },
        "file": {
            "class": "logging.FileHandler",
            "filename": os.getenv("BRM_LOG_FILE", "brm_core.log"),
            "formatter": "standard",
            "level": os.getenv("BRM_LOG_LEVEL", "INFO")
        }
    },
    "root": {
        "handlers": ["console", "file"],
        "level": "DEBUG"
    }
}

logging.config.dictConfig(LOG_CONFIG)
logger = logging.getLogger("brm_master")

##############################################################################
# (2) DATABASE CONNECTION DIALOG
##############################################################################

class DatabaseConnectionDialog(QtWidgets.QDialog):
    """
    A dialog to select or enter an ODBC DSN / connection string.
    Enhanced to handle test-connection logic, error display, etc.
    """
    def __init__(self, parent=None):
        super().__init__(parent)
        self.connection = None
        self.setWindowTitle("DB Connection – Enhanced")
        self.resize(400, 200)

        main_layout = QVBoxLayout(self)
        lbl = QLabel("Select ODBC DSN or provide a custom connection string:")
        main_layout.addWidget(lbl)

        self.dsn_combo = QComboBox()
        try:
            dsn_dict = pyodbc.dataSources()
            # Filter DSNs that appear to be SQL Server or similar
            for dsn_name, driver in dsn_dict.items():
                if "SQL" in driver.upper() or "ODBC" in driver.upper():
                    self.dsn_combo.addItem(f"ODBC DSN: {dsn_name}", dsn_name)
        except Exception as ex:
            logger.error(f"Error listing DSNs: {ex}")
        main_layout.addWidget(self.dsn_combo)

        self.conn_str_edit = QLineEdit()
        self.conn_str_edit.setPlaceholderText("Or enter custom ODBC connection string (optional)")
        main_layout.addWidget(self.conn_str_edit)

        btn_h = QHBoxLayout()
        ok_btn = QPushButton("Connect")
        ok_btn.clicked.connect(self.accept)
        cancel_btn = QPushButton("Cancel")
        cancel_btn.clicked.connect(self.reject)
        btn_h.addWidget(ok_btn)
        btn_h.addWidget(cancel_btn)
        main_layout.addLayout(btn_h)

    def get_connection(self):
        """
        Attempts to create a pyodbc connection with user’s chosen DSN or custom string.
        Returns None if it fails.
        """
        custom_str = self.conn_str_edit.text().strip()
        if custom_str:
            conn_str = custom_str
        else:
            sel = self.dsn_combo.currentData()
            if not sel:
                QMessageBox.critical(self, "Error", "No DSN or custom connection string provided.")
                return None
            conn_str = f"DSN={sel};Trusted_Connection=yes;"
        try:
            conn = pyodbc.connect(conn_str)
            logger.info("Database connection established successfully.")
            return conn
        except Exception as ex:
            QMessageBox.critical(self, "Connection Error", str(ex))
            logger.error(f"DB connection failed: {ex}")
            return None

    def accept(self):
        test_conn = self.get_connection()
        if test_conn:
            self.connection = test_conn
            super().accept()
        else:
            # Remain in the dialog for user correction
            pass

##############################################################################
# (3) BASIC & ADVANCED DB HELPERS
##############################################################################

def fetch_all_dict(cursor):
    """
    Fetch all rows into a list of dicts if description is present, else raw rows.
    """
    rows = cursor.fetchall()
    if cursor.description:
        colnames = [d[0] for d in cursor.description]
        out = []
        for r_ in rows:
            out.append(dict(zip(colnames, r_)))
        return out
    else:
        return rows

def fetch_one_dict(cursor):
    """
    Fetch the next row as dict (if present) or None.
    """
    row = cursor.fetchone()
    if row and cursor.description:
        colnames = [d[0] for d in cursor.description]
        return dict(zip(colnames, row))
    return None

def insert_audit_log(conn, action, table_name, record_id, actor, old_data=None, new_data=None):
    """
    Insert an audit record into BRM_AUDIT_LOG with optional old/new data as JSON.
    Enhanced: Incorporates robust error handling.
    """
    try:
        c = conn.cursor()
        c.execute("""
            INSERT INTO BRM_AUDIT_LOG(
                ACTION, TABLE_NAME, RECORD_ID, ACTION_BY,
                OLD_DATA, NEW_DATA, ACTION_TIMESTAMP
            )
            VALUES(?,?,?,?,?,?,GETDATE())
        """, (
            action, table_name,
            str(record_id) if record_id else None,
            actor,
            json.dumps(old_data) if old_data else None,
            json.dumps(new_data) if new_data else None
        ))
        conn.commit()
        logger.debug(f"Audit log inserted => action={action}, table={table_name}, record_id={record_id}, actor={actor}")
    except Exception as ex:
        logger.error(f"Error inserting audit log: {ex}")

def create_multistep_approvals(conn, rule_id, initiated_by):
    """
    Creates multi-step approval process records for a rule in BRM_RULE_APPROVALS.
    Enhanced with possibility for parallel or staged approvals in an advanced scenario.

    In a real scenario, the logic might read from a config table to see which groups must 
    approve a rule referencing certain tables or with a certain critical scope. 
    This code is a simplified example with 2 sequential steps.
    """
    try:
        c = conn.cursor()
        # Example: 2-stage, first "OwnerGroup" then "GlobalApprovers"
        # Could also handle parallel steps if needed.
        # For advanced parallel logic, we'd set something like APPROVAL_STAGE=1 for multiple rows, 
        # and the rule moves to next stage only if all parallel stage=1 approvals are done, etc.
        c.execute("SELECT OWNER_GROUP, CRITICAL_SCOPE FROM BRM_RULES WHERE RULE_ID=?", (rule_id,))
        row = c.fetchone()
        if not row:
            raise ValueError("Rule not found for approvals")
        owner_group, crit_scope = row
        # Insert stage 1
        c.execute("""
            INSERT INTO BRM_RULE_APPROVALS(RULE_ID, GROUP_NAME, USERNAME, APPROVED_FLAG, APPROVAL_STAGE, APPROVAL_TYPE)
            VALUES(?, ?, ?, 0, 1, 'SERIAL')
        """, (rule_id, owner_group, initiated_by))
        # Insert stage 2 (global or final approver):
        c.execute("""
            INSERT INTO BRM_RULE_APPROVALS(RULE_ID, GROUP_NAME, USERNAME, APPROVED_FLAG, APPROVAL_STAGE, APPROVAL_TYPE)
            VALUES(?, ?, ?, 0, 2, 'SERIAL')
        """, (rule_id, "GlobalApprovers", initiated_by))
        conn.commit()
        logger.info(f"Multi‑step approvals created for rule {rule_id} by {initiated_by}")
    except Exception as ex:
        conn.rollback()
        logger.error(f"Error creating multi‑step approvals for rule {rule_id}: {ex}")
        raise

##############################################################################
# (4) EMAIL NOTIFIER & COLLABORATION
##############################################################################

class EmailNotifier:
    """
    Sends emails via SMTP, reading credentials from environment variables
    or falling back to placeholder defaults.
    Provides detailed logging and error reporting.
    """
    def __init__(self):
        self.smtp_server = os.getenv("BRM_SMTP_SERVER", "smtp.example.com")
        self.smtp_port = int(os.getenv("BRM_SMTP_PORT", 587))
        self.smtp_username = os.getenv("BRM_SMTP_USERNAME", "your_smtp_user")
        self.smtp_password = os.getenv("BRM_SMTP_PASSWORD", "your_smtp_pass")
        self.sender_email = os.getenv("BRM_SENDER_EMAIL", "noreply@example.com")

    def send_email(self, subject: str, body: str, recipients: list):
        if not recipients:
            logger.warning("No recipients provided for email.")
            return
        try:
            msg = MIMEText(body, 'plain')
            msg['Subject'] = subject
            msg['From'] = self.sender_email
            msg['To'] = ", ".join(recipients)

            smtp = smtplib.SMTP(self.smtp_server, self.smtp_port)
            smtp.starttls()
            smtp.login(self.smtp_username, self.smtp_password)
            smtp.sendmail(self.sender_email, recipients, msg.as_string())
            smtp.quit()
            logger.info(f"Email sent to {recipients}")
        except Exception as e:
            logger.error(f"Error sending email to {recipients}: {e}")
            raise

class CollaborationManager(QtCore.QObject):
    """
    Periodically polls a COLLABORATION_LOGS table to retrieve new messages.
    Enhanced for potential multi-thread or mention logic in future expansions.
    """
    newMessage = QtCore.pyqtSignal(dict)

    def __init__(self, connection, poll_ms=5000, parent=None):
        super().__init__(parent)
        self.connection = connection
        self.poll_ms = poll_ms
        self.timer = QTimer(self)
        self.timer.timeout.connect(self.poll_messages)
        self.last_timestamp = None
        self.timer.start(self.poll_ms)

    def poll_messages(self):
        c = self.connection.cursor()
        try:
            if self.last_timestamp:
                c.execute("""
                    SELECT MESSAGE, SENDER, TIMESTAMP 
                    FROM COLLABORATION_LOGS 
                    WHERE TIMESTAMP > ? 
                    ORDER BY TIMESTAMP ASC
                """, (self.last_timestamp,))
            else:
                c.execute("SELECT MESSAGE, SENDER, TIMESTAMP FROM COLLABORATION_LOGS ORDER BY TIMESTAMP ASC")
            rows = c.fetchall()
            if rows:
                for row in rows:
                    message, sender, ts = row
                    self.newMessage.emit({"message": message, "sender": sender, "timestamp": ts})
                    self.last_timestamp = ts
        except Exception as ex:
            logger.error(f"Error polling collaboration messages: {ex}")

##############################################################################
# (5) LOCK MANAGER (UNIFIED APPROACH)
##############################################################################

class LockManager:
    """
    Stores locks in BRM_RULE_LOCKS with expiry.
    Provides:
      • auto_unlock_expired_locks
      • rule_current_lock_owner
      • lock_rule_for_edit
      • unlock_rule_for_edit
    Enhanced with more robust error handling and logging.
    """

    @staticmethod
    def auto_unlock_expired_locks(conn):
        now = datetime.now()
        c = conn.cursor()
        try:
            c.execute("""
                UPDATE BRM_RULE_LOCKS
                SET ACTIVE_LOCK=0
                WHERE ACTIVE_LOCK=1
                  AND EXPIRY_TIMESTAMP < ?
            """, (now,))
            rc = c.rowcount
            conn.commit()
            if rc > 0:
                logger.info(f"Auto-unlocked {rc} expired rule locks.")
        except Exception as ex:
            logger.error(f"Error auto-unlocking expired locks: {ex}")

    @staticmethod
    def rule_current_lock_owner(conn, rule_id):
        c = conn.cursor()
        c.execute("""
            SELECT LOCKED_BY, LOCK_TIMESTAMP, EXPIRY_TIMESTAMP, FORCE_LOCK
            FROM BRM_RULE_LOCKS
            WHERE RULE_ID=? AND ACTIVE_LOCK=1
        """, (rule_id,))
        row = c.fetchone()
        if not row:
            return None
        locked_by, lts, et, fflag = row
        now = datetime.now()
        if et and now > et:
            # Lock expired
            try:
                c2 = conn.cursor()
                c2.execute("UPDATE BRM_RULE_LOCKS SET ACTIVE_LOCK=0 WHERE RULE_ID=? AND ACTIVE_LOCK=1", (rule_id,))
                conn.commit()
            except Exception as ex:
                logger.error(f"Error expiring lock for rule {rule_id}: {ex}")
            return None
        return (locked_by, lts, et, fflag)

    @staticmethod
    def lock_rule_for_edit(conn, rule_id, user_id, force=False, lock_minutes=30):
        LockManager.auto_unlock_expired_locks(conn)
        info = LockManager.rule_current_lock_owner(conn, rule_id)
        now = datetime.now()
        expiry = now + timedelta(minutes=lock_minutes)
        c = conn.cursor()
        if info is not None:
            locked_by, old_ts, old_exp, fflag = info
            if locked_by == user_id:
                # Refresh existing lock
                c.execute("""
                    UPDATE BRM_RULE_LOCKS
                    SET LOCK_TIMESTAMP=?, EXPIRY_TIMESTAMP=?, FORCE_LOCK=?
                    WHERE RULE_ID=? AND ACTIVE_LOCK=1
                """, (now, expiry, 1 if force else 0, rule_id))
                conn.commit()
                logger.debug(f"Lock for rule {rule_id} refreshed by {user_id}. Force={force}")
                return
            else:
                if not force:
                    raise ValueError(f"Rule {rule_id} is locked by {locked_by}.")
                else:
                    # Force override
                    c.execute("UPDATE BRM_RULE_LOCKS SET ACTIVE_LOCK=0 WHERE RULE_ID=? AND ACTIVE_LOCK=1", (rule_id,))
                    c.execute("""
                        INSERT INTO BRM_RULE_LOCKS(
                          RULE_ID, LOCKED_BY, LOCK_TIMESTAMP, EXPIRY_TIMESTAMP,
                          FORCE_LOCK, ACTIVE_LOCK
                        )
                        VALUES(?,?,?,?,?,1)
                    """, (rule_id, user_id, now, expiry, 1))
                    conn.commit()
                    logger.debug(f"Rule {rule_id} forcibly re-locked by {user_id}.")
                    return
        else:
            # No active lock
            c.execute("""
                INSERT INTO BRM_RULE_LOCKS(
                  RULE_ID, LOCKED_BY, LOCK_TIMESTAMP, EXPIRY_TIMESTAMP,
                  FORCE_LOCK, ACTIVE_LOCK
                )
                VALUES(?,?,?,?,?,1)
            """, (rule_id, user_id, now, expiry, 1 if force else 0))
            conn.commit()
            logger.debug(f"Rule {rule_id} locked by {user_id}, force={force}.")

    @staticmethod
    def unlock_rule_for_edit(conn, rule_id, user_id, force=False):
        LockManager.auto_unlock_expired_locks(conn)
        info = LockManager.rule_current_lock_owner(conn, rule_id)
        if not info:
            return  # not locked or already expired
        locked_by, lts, et, fflag = info
        if locked_by != user_id and not force:
            raise ValueError(f"Rule {rule_id} is locked by {locked_by}, cannot unlock.")
        c = conn.cursor()
        c.execute("""
            UPDATE BRM_RULE_LOCKS
            SET ACTIVE_LOCK=0
            WHERE RULE_ID=? AND ACTIVE_LOCK=1
        """, (rule_id,))
        conn.commit()
        logger.debug(f"Rule {rule_id} unlocked by user {user_id}, force={force}.")
##############################################################################
# (6) ADVANCED SQL PARSER
##############################################################################

def detect_operation_type(sql_text: str) -> str:
    """
    Return one of: INSERT, UPDATE, DELETE, SELECT, or OTHER.
    Enhanced: Strips comments/whitespace before determining the operation type.
    """
    cleaned = re.sub(r'(--.*?$)|(/\*.*?\*/)', '', sql_text or '', flags=re.MULTILINE | re.DOTALL).lstrip()
    up = cleaned.upper()
    if up.startswith("INSERT"):
        return "INSERT"
    elif up.startswith("UPDATE"):
        return "UPDATE"
    elif up.startswith("DELETE"):
        return "DELETE"
    elif up.startswith("SELECT"):
        return "SELECT"
    else:
        return "OTHER"


def parse_sql_dependencies(sql_text: str):
    """
    Use sqlparse to identify table references, columns, CTE usage, etc.
    Returns a dict with keys 'tables', 'cte_tables', 'alias_map', 'columns'.
    Enhanced with more robust error handling for edge cases.
    """
    try:
        statements = sqlparse.parse(sql_text)
    except Exception as ex:
        logger.error(f"SQL parsing error: {ex}")
        return {"tables": [], "cte_tables": [], "alias_map": {}, "columns": []}

    all_tables = []
    cte_info = []
    alias_map = {}
    columns = []

    for stmt in statements:
        ctes = _extract_with_clauses(stmt)
        for cName, cRefs in ctes.items():
            cte_info.append((cName, cRefs))

        main_refs, main_alias = _extract_main_from(stmt.tokens, set(ctes.keys()))
        all_tables.extend(main_refs)
        alias_map.update(main_alias)

        col_refs = _extract_columns(stmt)
        columns.extend(col_refs)

    unique_tables = list({x[1] for x in all_tables if x[1] is not None})
    return {
        "tables": unique_tables,
        "cte_tables": cte_info,
        "alias_map": alias_map,
        "columns": columns
    }

# Helper functions for parse_sql_dependencies

def _extract_with_clauses(statement):
    tokens = list(statement.tokens)
    i = 0
    cte_map = {}
    from sqlparse.tokens import Keyword

    while i < len(tokens):
        tk = tokens[i]
        if tk.ttype is Keyword and tk.value.upper() == "WITH":
            i += 1
            i = _parse_cte_block(tokens, i, cte_map)
            continue
        i += 1
    return cte_map

def _parse_cte_block(tokens, i, cte_map):
    from sqlparse.sql import Identifier, Parenthesis
    from sqlparse.tokens import Keyword

    while i < len(tokens):
        tk = tokens[i]
        if isinstance(tk, Identifier):
            cte_name = tk.get_real_name()
            i += 1
            i = _parse_cte_as_clause(tokens, i, cte_name, cte_map)
        elif tk.ttype is Keyword and tk.value.upper() in ("SELECT", "INSERT", "UPDATE", "DELETE"):
            return i
        else:
            i += 1
    return i

def _parse_cte_as_clause(tokens, i, cte_name, cte_map):
    from sqlparse.sql import Parenthesis
    while i < len(tokens):
        tk = tokens[i]
        if tk.value.upper() == "AS":
            i += 1
            if i < len(tokens):
                sub = tokens[i]
                if isinstance(sub, Parenthesis):
                    sub_refs = _extract_subselect_tokens(sub.tokens)
                    cte_map[cte_name] = sub_refs
                    i += 1
                    return i
        else:
            i += 1
    return i

def _extract_subselect_tokens(tokens):
    from sqlparse.sql import IdentifierList, Identifier
    from sqlparse.tokens import Keyword
    results = []
    from_seen = False
    i = 0
    while i < len(tokens):
        tk = tokens[i]
        if hasattr(tk, "is_group") and tk.is_group and _is_subselect(tk):
            results.extend(_extract_subselect_tokens(tk.tokens))
        if tk.ttype is Keyword:
            upv = tk.value.upper()
            if upv in ("FROM", "JOIN", "INNER JOIN", "LEFT JOIN", "RIGHT JOIN", "FULL JOIN"):
                from_seen = True
            else:
                from_seen = False
        if from_seen:
            if isinstance(tk, IdentifierList):
                for ident in tk.get_identifiers():
                    st = _parse_identifier(ident, set())
                    results.append(st)
            elif isinstance(tk, Identifier):
                st = _parse_identifier(tk, set())
                results.append(st)
        i += 1
    return results

def _is_subselect(token):
    from sqlparse.tokens import DML
    if not token.is_group:
        return False
    for sub in token.tokens:
        if sub.ttype is DML and sub.value.upper() == "SELECT":
            return True
    return False

def _extract_main_from(tokenlist, known_cte_names):
    from sqlparse.tokens import Keyword
    from sqlparse.sql import IdentifierList, Identifier
    results = []
    alias_map = {}
    tokens = list(tokenlist)
    from_seen = False
    i = 0
    while i < len(tokens):
        tk = tokens[i]
        if hasattr(tk, "is_group") and tk.is_group and _is_subselect(tk):
            results.extend(_extract_subselect_tokens(tk.tokens))
        if tk.ttype is Keyword:
            upv = tk.value.upper()
            if upv in ("FROM", "JOIN", "INNER JOIN", "LEFT JOIN", "RIGHT JOIN", "FULL JOIN"):
                from_seen = True
            else:
                from_seen = False
        if from_seen:
            if isinstance(tk, IdentifierList):
                for ident in tk.get_identifiers():
                    st = _parse_identifier(ident, known_cte_names)
                    results.append(st)
                    if st[2]:
                        alias_map[st[2]] = (st[0], st[1])
            elif isinstance(tk, Identifier):
                st = _parse_identifier(tk, known_cte_names)
                results.append(st)
                if st[2]:
                    alias_map[st[2]] = (st[0], st[1])
        i += 1
    return (results, alias_map)

def _parse_identifier(ident, known_cte_names):
    alias = ident.get_alias()
    real_name = ident.get_real_name()
    schema = ident.get_parent_name()
    if real_name and real_name.upper() in (n.upper() for n in known_cte_names):
        return (None, f"(CTE) {real_name}", alias, False)
    return (schema, real_name, alias, False)

def _extract_columns(statement):
    from sqlparse.tokens import DML, Keyword
    from sqlparse.sql import IdentifierList, Identifier, Parenthesis
    results = []
    tokens = list(statement.tokens)
    i = 0
    while i < len(tokens):
        tk = tokens[i]
        if tk.ttype is DML:
            upv = tk.value.upper()
            if upv == "SELECT":
                results.extend(_parse_select_list(tokens, i + 1))
            elif upv in ("INSERT", "UPDATE"):
                results.extend(_parse_dml_columns(tokens, i, upv))
        i += 1
    return results

def _parse_select_list(tokens, start_idx):
    from sqlparse.tokens import Keyword
    from sqlparse.sql import IdentifierList, Identifier
    columns = []
    i = start_idx
    while i < len(tokens):
        tk = tokens[i]
        if tk.ttype is Keyword:
            upv = tk.value.upper()
            if upv in ("FROM", "JOIN", "WHERE", "GROUP", "ORDER", "UNION", "INTERSECT"):
                break
        if isinstance(tk, IdentifierList):
            for ident in tk.get_identifiers():
                nm = ident.get_name()
                if nm and nm.upper() not in ("DISTINCT", "TOP", "ALL"):
                    columns.append(nm)
        elif isinstance(tk, Identifier):
            nm = tk.get_name()
            if nm and nm.upper() not in ("DISTINCT", "TOP", "ALL"):
                columns.append(nm)
        i += 1
    return columns

def _parse_dml_columns(tokens, start_idx, dml_word):
    from sqlparse.tokens import Keyword
    from sqlparse.sql import Parenthesis, IdentifierList, Identifier
    columns = []
    if dml_word == "INSERT":
        i = start_idx
        while i < len(tokens):
            tk = tokens[i]
            if tk.is_group and isinstance(tk, Parenthesis):
                for sub in tk.tokens:
                    if isinstance(sub, IdentifierList):
                        for ident in sub.get_identifiers():
                            columns.append(ident.get_name())
                    elif isinstance(sub, Identifier):
                        columns.append(sub.get_name())
                return columns
            i += 1
    elif dml_word == "UPDATE":
        found_set = False
        i = start_idx
        while i < len(tokens):
            tk = tokens[i]
            if tk.ttype is Keyword and tk.value.upper() == "SET":
                found_set = True
                i += 1
                columns.extend(_parse_update_set_list(tokens, i))
                break
            i += 1
    return columns

def _parse_update_set_list(tokens, start_i):
    from sqlparse.tokens import Keyword
    from sqlparse.sql import Identifier
    cols = []
    i = start_i
    while i < len(tokens):
        tk = tokens[i]
        if tk.ttype is Keyword and tk.value.upper() in ("WHERE", "FROM"):
            break
        if isinstance(tk, Identifier):
            cols.append(tk.get_name())
        i += 1
    return cols

##############################################################################
# (7) LOGIN DIALOG (PLAIN TEXT PASSWORD COMPARISON)
##############################################################################

class LoginDialog(QtWidgets.QDialog):
    """
    Minimal user/password dialog that checks the USERS table.
    Uses plain-text password comparison, as requested.
    Enhanced with potential RBAC logic placeholders.
    """
    def __init__(self, connection, parent=None):
        super().__init__(parent)
        self.connection = connection
        self.user_id = None
        self.user_group = None

        self.setWindowTitle("Login – Enhanced")
        self.resize(300, 150)

        main_layout = QVBoxLayout(self)
        self.user_edit = QLineEdit()
        self.user_edit.setPlaceholderText("Username")
        main_layout.addWidget(QLabel("Username:"))
        main_layout.addWidget(self.user_edit)

        self.pass_edit = QLineEdit()
        self.pass_edit.setPlaceholderText("Password")
        self.pass_edit.setEchoMode(QLineEdit.Password)
        main_layout.addWidget(QLabel("Password:"))
        main_layout.addWidget(self.pass_edit)

        login_btn = QPushButton("Login")
        login_btn.clicked.connect(self.do_login)
        main_layout.addWidget(login_btn)

        self.setLayout(main_layout)

    def do_login(self):
        un = self.user_edit.text().strip()
        pw = self.pass_edit.text().strip()

        if not un or not pw:
            QMessageBox.warning(self, "Error", "Both username and password are required.")
            return

        try:
            logger.debug(f"Attempting login for user: {un}")
            c = self.connection.cursor()
            c.execute("""
                SELECT USER_ID, USER_GROUP 
                FROM USERS
                WHERE USERNAME = ? AND PASSWORD = ?
            """, (un, pw))
            row = c.fetchone()
            if row:
                self.user_id = row[0]
                self.user_group = row[1]
                logger.info(f"User {un} logged in successfully. user_id={self.user_id}, user_group={self.user_group}")
                self.accept()
            else:
                logger.warning(f"Login failed for user: {un} - Invalid credentials.")
                QMessageBox.warning(self, "Login Failed", "Invalid credentials.")
        except Exception as ex:
            logger.exception("Error during login:")
            QMessageBox.critical(self, "Database Error", f"An error occurred during login:\n{ex}")

##############################################################################
# (8) ONBOARDING WIZARD (FULLY IMPLEMENTED)
##############################################################################

class OnboardingWizard(QDialog):
    """
    A multi-step wizard for brand-new users:
      Step 1) create a group
      Step 2) create a rule
      Step 3) schedule it
    Then done. Provides explicit guidance with no placeholders.
    Optionally extended with bubble tours / tooltips.
    """
    def __init__(self, connection, parent=None):
        super().__init__(parent)
        self.connection = connection
        self.current_step = 0
        self.setWindowTitle("Onboarding Wizard – Enhanced")
        self.resize(400, 250)
        main_l = QVBoxLayout(self)

        self.label = QLabel("Welcome to the BRM Tool! This wizard helps new users complete initial setup.")
        main_l.addWidget(self.label)

        self.next_btn = QPushButton("Next")
        self.next_btn.clicked.connect(self.advance_step)
        main_l.addWidget(self.next_btn)

        self.setLayout(main_l)

    def advance_step(self):
        self.current_step += 1
        if self.current_step == 1:
            self.label.setText("Step 1: Navigate to 'Group Management' to create a new group.\n"
                               "For example, name it after your department or risk domain.")
        elif self.current_step == 2:
            self.label.setText("Step 2: Navigate to 'Business Rules' to add a new rule.\n"
                               "For instance, create a basic threshold check rule.")
        elif self.current_step == 3:
            self.label.setText("Step 3: Navigate to 'Scheduling' to schedule your new rule to run automatically.")
        else:
            self.label.setText("Setup complete! Enjoy using the BRM Tool.")
            self.accept()

##############################################################################
# (9) sync_metadata_improved
##############################################################################

def sync_metadata_improved(conn):
    """
    Marks missing table references in BRM_RULE_TABLE_DEPENDENCIES by prefixing 'MISSING_'.
    Provides comprehensive error handling and logging.
    """
    try:
        c = conn.cursor()
        c.execute("""
            SELECT s.name AS schema_name, t.name AS table_name
            FROM sys.tables t
            JOIN sys.schemas s ON t.schema_id=s.schema_id
            ORDER BY s.name, t.name
        """)
        actual_tables = set()
        for row in c.fetchall():
            full_n = (f"{row[0]}.{row[1]}").lower()
            actual_tables.add(full_n)

        c.execute("SELECT DEPENDENCY_ID, DATABASE_NAME, TABLE_NAME FROM BRM_RULE_TABLE_DEPENDENCIES")
        deps = c.fetchall()
        missing_count = 0
        for (dep_id, dbn, tbl) in deps:
            if not tbl:
                continue
            low_tbl = tbl.lower().strip()
            if "." not in low_tbl:
                low_tbl = f"dbo.{low_tbl}"
            if low_tbl not in actual_tables:
                c2 = conn.cursor()
                c2.execute("""
                    UPDATE BRM_RULE_TABLE_DEPENDENCIES
                    SET TABLE_NAME='MISSING_' + TABLE_NAME
                    WHERE DEPENDENCY_ID=?
                """, (dep_id,))
                missing_count += 1

        conn.commit()
        msg = (f"Metadata sync complete. Found {len(actual_tables)} real DB tables.\n"
               f"Scanned {len(deps)} dependencies.\n"
               f"Marked {missing_count} references as 'MISSING_'.")
        logger.info(msg)
        return msg
    except Exception as ex:
        logger.error(f"Sync metadata error: {ex}")
        return f"Sync error: {ex}"
        
###############################################################################
# PART 2: ADVANCED FUNCTIONALITY
# BFS-based rule execution, collaboration, multi-step approvals, etc.
###############################################################################

def run_data_validations(conn):
    """
    Executes all data validations defined in the DATA_VALIDATIONS table.
    Each validation can be of type:
      - "NOT NULL": checks if specified column has NULLs
      - "RANGE": expects params like "min=0;max=100"
      - "UNIQUE": checks if specified column has duplicates
    Results are logged into DATA_VALIDATION_LOGS.
    """
    cursor = conn.cursor()

    try:
        cursor.execute("""
            SELECT VALIDATION_ID, TABLE_NAME, COLUMN_NAME, VALIDATION_TYPE, PARAMS
            FROM DATA_VALIDATIONS
        """)
        validations = cursor.fetchall()
    except Exception as e:
        logger.error("Error fetching validations: %s", e)
        return

    for val in validations:
        validation_id, table_name, column_name, validation_type, params = val
        result_flag = "PASS"
        details = ""
        try:
            if validation_type.upper() == "NOT NULL":
                query = f"SELECT COUNT(*) FROM {table_name} WHERE {column_name} IS NULL"
                cursor.execute(query)
                count = cursor.fetchone()[0]
                if count > 0:
                    result_flag = "FAIL"
                    details = f"{count} rows have NULL in column '{column_name}'."
                else:
                    details = f"All rows have non-NULL values in column '{column_name}'."
            elif validation_type.upper() == "RANGE":
                # Expecting params like "min=0;max=100"
                min_val = None
                max_val = None
                if params:
                    for part in params.split(";"):
                        part = part.strip().lower()
                        if part.startswith("min="):
                            min_val = float(part.split("=")[1])
                        elif part.startswith("max="):
                            max_val = float(part.split("=")[1])
                if min_val is None or max_val is None:
                    result_flag = "FAIL"
                    details = "Invalid parameters for RANGE validation."
                else:
                    query = f"""
                        SELECT COUNT(*) FROM {table_name}
                        WHERE TRY_CAST({column_name} AS FLOAT) IS NOT NULL
                          AND (TRY_CAST({column_name} AS FLOAT) < {min_val}
                               OR TRY_CAST({column_name} AS FLOAT) > {max_val})
                    """
                    cursor.execute(query)
                    count = cursor.fetchone()[0]
                    if count > 0:
                        result_flag = "FAIL"
                        details = f"{count} rows out of range [{min_val}, {max_val}]."
                    else:
                        details = f"All rows within range [{min_val}, {max_val}]."
            elif validation_type.upper() == "UNIQUE":
                query = f"""
                    SELECT COUNT(*) FROM (
                        SELECT {column_name}, COUNT(*) AS cnt
                        FROM {table_name}
                        GROUP BY {column_name}
                        HAVING COUNT(*) > 1
                    ) AS duplicates
                """
                cursor.execute(query)
                count = cursor.fetchone()[0]
                if count > 0:
                    result_flag = "FAIL"
                    details = f"{count} duplicate value(s) found in column '{column_name}'."
                else:
                    details = f"All values in column '{column_name}' are unique."
            else:
                result_flag = "FAIL"
                details = f"Validation type '{validation_type}' not implemented."
        except Exception as ex:
            result_flag = "FAIL"
            details = f"Error during validation: {ex}"

        # Log the result in DATA_VALIDATION_LOGS
        try:
            cursor.execute("""
                INSERT INTO DATA_VALIDATION_LOGS(VALIDATION_ID, RESULT_FLAG, DETAILS, VALIDATION_TIMESTAMP)
                VALUES(?, ?, ?, GETDATE())
            """, (validation_id, result_flag, details))
            conn.commit()
        except Exception as log_ex:
            logger.error(f"Error logging validation {validation_id}: {log_ex}")


###############################################################################
# BFS RULE EXECUTION (UNIFIED)
###############################################################################

def execute_rules_unified_bfs(conn, dry_run=False):
    """
    Executes rules using a BFS approach:
      1. Runs data validations first (if desired).
      2. Processes each rule in BFS order (roots to children).
      3. Skips child rules if a critical/global rule fails.
    Logs execution outcomes into RULE_EXECUTION_LOGS.
    Returns two lists: (executed_rule_ids, skipped_rule_ids).
    """
    run_data_validations(conn)  # Potentially optional

    adjacency, roots, parent_map = load_rule_relationships(conn)
    rule_lookup = get_all_rules_map(conn)
    executed = []
    skipped = set()
    queue = list(roots)

    while queue:
        rid = queue.pop(0)
        if rid in skipped:
            continue
        if rid not in rule_lookup:
            skipped.add(rid)
            continue

        info = rule_lookup[rid]
        ok, msg, rec_count = run_single_rule_in_transaction(conn, info, is_dry_run=dry_run)
        insert_rule_execution_log(conn, rid, ok, msg, rec_count)

        if ok:
            executed.append(rid)
            if rid in adjacency:
                for child_id in adjacency[rid]:
                    if child_id not in skipped:
                        queue.append(child_id)
        else:
            # For critical or global rules, skip all descendants
            is_crit = (info.get("CRITICAL_RULE", 0) == 1 or info.get("IS_GLOBAL", 0) == 1)
            if is_crit and rid in adjacency:
                for ch in adjacency[rid]:
                    skip_all_descendants(ch, adjacency, skipped)
            if rid in adjacency:
                for ch in adjacency[rid]:
                    skip_all_descendants(ch, adjacency, skipped)
            skipped.add(rid)

    return (executed, list(skipped))


def skip_all_descendants(start_id, adjacency, skipped):
    """
    Recursively marks all descendants reachable from start_id as skipped.
    """
    stack = [start_id]
    while stack:
        cur = stack.pop()
        if cur in skipped:
            continue
        skipped.add(cur)
        if cur in adjacency:
            for child in adjacency[cur]:
                if child not in skipped:
                    stack.append(child)


def load_rule_relationships(conn):
    """
    Constructs adjacency list for BFS: PARENT_RULE_ID -> [child rule IDs].
    Also handles conflict links, global-critical links, composite references (if needed).
    Returns (adjacency, roots, parent_map).
    """
    c = conn.cursor()
    c.execute("SELECT RULE_ID, PARENT_RULE_ID FROM BRM_RULES")
    rows = c.fetchall()
    adjacency = {}
    parent_map = {}
    all_ids = set()
    for (rid, pid) in rows:
        all_ids.add(rid)
        if pid:
            adjacency.setdefault(pid, set()).add(rid)
            parent_map[rid] = pid

    # Conflicts are bidirectional, but for BFS we can handle them differently if needed.
    # For example:
    c.execute("SELECT RULE_ID1, RULE_ID2 FROM RULE_CONFLICTS")
    conflict_rows = c.fetchall()
    for (r1, r2) in conflict_rows:
        adjacency.setdefault(r1, set()).add(r2)
        adjacency.setdefault(r2, set()).add(r1)

    # Global-critical links (one directional: GCR_RULE_ID -> TARGET_RULE_ID).
    c.execute("SELECT GCR_RULE_ID, TARGET_RULE_ID FROM BRM_GLOBAL_CRITICAL_LINKS")
    gcrs = c.fetchall()
    for (gcr, tgt) in gcrs:
        adjacency.setdefault(gcr, set()).add(tgt)

    # Composite links (e.g., sub-rule -> composite rule).
    c.execute("SELECT COMPOSITE_RULE_ID, LOGIC_EXPR FROM COMPOSITE_RULES")
    comp_rows = c.fetchall()
    pat = re.compile(r"Rule(\d+)")
    for (comp_id, expr) in comp_rows:
        if expr:
            matches = pat.findall(expr)
            for m in matches:
                try:
                    sub_id = int(m)
                    adjacency.setdefault(sub_id, set()).add(comp_id)
                except Exception as ex:
                    logger.error(f"Error parsing composite rule expr: {ex}")
                    continue

    child_ids = set(parent_map.keys())
    roots = [rid for rid in all_ids if rid not in child_ids]
    return (adjacency, roots, parent_map)

def get_all_rules_map(conn):
    """
    Returns a dict: RULE_ID -> {rule data from BRM_RULES}.
    """
    c = conn.cursor()
    c.execute("SELECT * FROM BRM_RULES")
    rows = c.fetchall()
    colnames = [desc[0] for desc in c.description]
    rule_map = {}
    for row in rows:
        d = dict(zip(colnames, row))
        rule_map[d["RULE_ID"]] = d
    return rule_map


def run_single_rule_in_transaction(conn, rule_info, is_dry_run=False):
    """
    Executes a single rule in a transaction:
      - If OPERATION_TYPE = 'DECISION_TABLE', runs decision table logic
      - Else runs the RULE_SQL
    Returns (success_flag, message, record_count).
    Rolls back if is_dry_run or if result fails.
    """
    op_type = (rule_info.get("OPERATION_TYPE") or "").upper()
    rule_sql = rule_info.get("RULE_SQL") or ""
    dt_id = rule_info.get("DECISION_TABLE_ID")

    if op_type == "DECISION_TABLE":
        if not dt_id:
            return (False, "No DECISION_TABLE_ID provided", 0)
        return execute_decision_table(conn, dt_id, dry_run=True)

    c = conn.cursor()
    try:
        c.execute("BEGIN TRANSACTION")
    except Exception as ex:
        logger.error(f"Error beginning transaction for rule {rule_info.get('RULE_ID')}: {ex}")
        return (False, str(ex), 0)

    success = False
    msg = ""
    rec_count = 0

    try:
        c.execute(rule_sql)
        try:
            rows = c.fetchall()
        except Exception:
            rows = []
        rec_count = len(rows)
        if rows:
            val = rows[0][0]
            success = (val == 1)
            msg = f"Returned: {val}"
        else:
            success = True
            msg = "No rows returned => PASS"

        if is_dry_run or not success:
            c.execute("ROLLBACK")
        else:
            c.execute("COMMIT")
    except Exception as ex:
        try:
            c.execute("ROLLBACK")
        except Exception:
            pass
        success = False
        msg = str(ex)

    return (success, msg, rec_count)


def insert_rule_execution_log(conn, rule_id, pass_flag, message, record_count):
    """
    Inserts an execution log in RULE_EXECUTION_LOGS.
    """
    c = conn.cursor()
    try:
        c.execute("""
            INSERT INTO RULE_EXECUTION_LOGS(
              RULE_ID, EXECUTION_TIMESTAMP, PASS_FLAG,
              MESSAGE, RECORD_COUNT, EXECUTION_TIME_MS,
              CPU_USAGE, MEM_USAGE
            )
            VALUES(?, GETDATE(), ?, ?, ?, 0, 0, 0)
        """, (rule_id, 1 if pass_flag else 0, message, record_count))
        conn.commit()
    except Exception as ex:
        logger.error(f"Error inserting rule execution log for rule {rule_id}: {ex}")

###############################################################################
# DECISION TABLE EXECUTION
###############################################################################

def execute_decision_table(conn, dt_id, dry_run=True):
    """
    Runs a DECISION_TABLE:
      - Fetches DECISION_QUERY from DECISION_TABLES by dt_id
      - Executes it in a transaction
      - If first column of first row == 1 => PASS
      - Always rollbacks if dry_run
    Returns (success_flag, message, record_count).
    """
    c_dt = conn.cursor()
    c_dt.execute("SELECT DECISION_QUERY FROM DECISION_TABLES WHERE DECISION_TABLE_ID=?", (dt_id,))
    row = c_dt.fetchone()
    if not row:
        return (False, f"Decision table {dt_id} not found.", 0)

    decision_query = row[0] or ""
    c = conn.cursor()
    try:
        c.execute("BEGIN TRANSACTION")
    except Exception as ex:
        logger.error(f"Error starting transaction for decision table {dt_id}: {ex}")
        return (False, str(ex), 0)

    success = False
    msg = ""
    rec_count = 0

    try:
        c.execute(decision_query)
        try:
            rows = c.fetchall()
        except Exception:
            rows = []
        rec_count = len(rows)
        if rows:
            val = rows[0][0]
            success = (val == 1)
            msg = f"Decision table returned: {val}"
        else:
            success = True
            msg = "No rows returned => PASS"

        c.execute("ROLLBACK")  # always rollback for a "dry_run"
    except Exception as ex:
        try:
            c.execute("ROLLBACK")
        except:
            pass
        success = False
        msg = str(ex)

    return (success, msg, rec_count)


def insert_decision_table_log(conn, dt_id, pass_flag, message, record_count):
    """
    Inserts a log into DECISION_TABLE_EXEC_LOGS.
    """
    c = conn.cursor()
    try:
        c.execute("""
            INSERT INTO DECISION_TABLE_EXEC_LOGS(
              DECISION_TABLE_ID, EXEC_TIMESTAMP, PASS_FLAG, MESSAGE, RECORD_COUNT
            )
            VALUES(?, GETDATE(), ?, ?, ?)
        """, (dt_id, 1 if pass_flag else 0, message, record_count))
        conn.commit()
    except Exception as ex:
        logger.error(f"Error inserting decision table log for DT {dt_id}: {ex}")

###############################################################################
# DRY-RUN SIMULATIONS (CHAIN & CUSTOM GROUP)
###############################################################################

def dry_run_rule_sql(conn, sql_text):
    """
    Runs given SQL in a transaction, returns PASS if first column == 1, else FAIL.
    Always rollbacks. Used for "What-If" tests.
    """
    c = conn.cursor()
    try:
        c.execute("BEGIN TRANSACTION")
    except Exception as ex:
        return (False, f"Transaction start error: {ex}")
    success = True
    message = ""
    try:
        c.execute(sql_text)
        try:
            rows = c.fetchall()
        except Exception:
            rows = []
        if rows:
            val = rows[0][0]
            success = (val == 1)
            message = f"Returned: {val}"
        else:
            success = True
            message = "No rows returned => PASS"
        c.execute("ROLLBACK")
    except Exception as ex:
        try:
            c.execute("ROLLBACK")
        except:
            pass
        success = False
        message = str(ex)
    return (success, message)


def simulate_chain_bfs(conn, parent_rule_id):
    """
    Simulates a BFS chain dry-run from parent_rule_id:
      - For DECISION_TABLE rules, runs the table logic
      - For others, does dry_run_rule_sql
    Returns (executed, skipped) lists.
    """
    adjacency, roots, parent_map = load_rule_relationships(conn)
    c = conn.cursor()
    c.execute("SELECT RULE_ID, RULE_SQL, OPERATION_TYPE, DECISION_TABLE_ID FROM BRM_RULES")
    rows = c.fetchall()
    rule_map = {}
    for (rid, sql_, op_type, dt_id) in rows:
        rule_map[rid] = (sql_ or "", op_type or "OTHER", dt_id)
    executed = []
    skipped = set()
    queue = [parent_rule_id]

    while queue:
        rid = queue.pop(0)
        if rid in skipped:
            continue
        if rid not in rule_map:
            skipped.add(rid)
            continue

        sql_text, op_type, dt_id = rule_map[rid]
        if op_type.upper() == "DECISION_TABLE":
            ok, msg, _ = execute_decision_table(conn, dt_id, dry_run=True)
        else:
            ok, msg = dry_run_rule_sql(conn, sql_text)

        if ok:
            executed.append(rid)
            if rid in adjacency:
                for child_id in adjacency[rid]:
                    if child_id not in skipped:
                        queue.append(child_id)
        else:
            if rid in adjacency:
                for ch in adjacency[rid]:
                    skip_all_descendants(ch, adjacency, skipped)
            skipped.add(rid)
    return (executed, list(skipped))


def simulate_custom_group_rules(conn, custom_group_id):
    """
    Simulates each rule in a custom group via dry_run_rule_sql or decision_table logic.
    Returns (passed_ids, failed_ids).
    """
    c = conn.cursor()
    c.execute("SELECT RULE_ID FROM BRM_CUSTOM_GROUP_MEMBERS WHERE CUSTOM_GROUP_ID=?", (custom_group_id,))
    rule_ids = [r[0] for r in c.fetchall()]
    c.execute("SELECT RULE_ID, RULE_SQL, OPERATION_TYPE, DECISION_TABLE_ID FROM BRM_RULES")
    all_rows = c.fetchall()
    rule_map = {}
    for (rid, sql_, op_type, dt_id) in all_rows:
        rule_map[rid] = (sql_ or "", op_type or "OTHER", dt_id)

    passed = []
    failed = []

    for rid in rule_ids:
        if rid not in rule_map:
            failed.append(rid)
            continue
        sql_text, op_type, dt_id = rule_map[rid]
        if op_type.upper() == "DECISION_TABLE":
            ok, msg, _ = execute_decision_table(conn, dt_id, dry_run=True)
            if ok:
                passed.append(rid)
            else:
                failed.append(rid)
        else:
            ok, msg = dry_run_rule_sql(conn, sql_text)
            if ok:
                passed.append(rid)
            else:
                failed.append(rid)

    return (passed, failed)

###############################################################################
# PART 3: UI FOR ADVANCED SCHEDULING, SIMULATIONS, ETC.
###############################################################################

class EnhancedScheduleDialog(QtWidgets.QDialog):
    """
    Fully functional scheduling dialog to pick a rule, date/time,
    and optional run_validations. Inserts into RULE_SCHEDULES.
    """
    def __init__(self, connection, parent=None):
        super().__init__(parent)
        self.connection = connection
        self.setWindowTitle("Enhanced Scheduling")
        self.resize(400, 300)
        layout = QVBoxLayout(self)

        form = QFormLayout()
        self.rule_combo = QComboBox()
        c = connection.cursor()
        c.execute("SELECT RULE_ID, RULE_NAME FROM BRM_RULES ORDER BY RULE_ID")
        for (rid, rn) in c.fetchall():
            disp = f"{rid} - {rn}"
            self.rule_combo.addItem(disp, rid)
        form.addRow("Select Rule:", self.rule_combo)

        self.calendar = QCalendarWidget()
        self.calendar.setSelectedDate(QtCore.QDate.currentDate())
        self.calendar.setGridVisible(True)
        form.addRow("Select Date:", self.calendar)

        self.time_edit = QTimeEdit(QtCore.QTime.currentTime())
        self.time_edit.setDisplayFormat("HH:mm:ss")
        form.addRow("Select Time:", self.time_edit)

        self.run_val_checkbox = QCheckBox("Run Data Validations Before Rule?")
        self.run_val_checkbox.setChecked(False)
        form.addRow(self.run_val_checkbox)

        layout.addLayout(form)

        btn_layout = QHBoxLayout()
        sch_btn = QPushButton("Schedule")
        sch_btn.clicked.connect(self.do_schedule)
        btn_layout.addWidget(sch_btn)

        close_btn = QPushButton("Close")
        close_btn.clicked.connect(self.close)
        btn_layout.addWidget(close_btn)
        btn_layout.addStretch()
        layout.addLayout(btn_layout)
        self.setLayout(layout)

    def do_schedule(self):
        rid = self.rule_combo.currentData()
        date_str = self.calendar.selectedDate().toString("yyyy-MM-dd")
        time_str = self.time_edit.time().toString("HH:mm:ss")
        dt_str = f"{date_str} {time_str}"
        run_val_flag = 1 if self.run_val_checkbox.isChecked() else 0

        c = self.connection.cursor()
        try:
            c.execute("""
                INSERT INTO RULE_SCHEDULES(
                  RULE_ID, SCHEDULE_TIME, STATUS, CREATED_TIMESTAMP, RUN_DATA_VALIDATIONS
                )
                VALUES(?, ?, 'Scheduled', GETDATE(), ?)
            """, (rid, dt_str, run_val_flag))
            self.connection.commit()
            QMessageBox.information(self, "Scheduled", f"Rule {rid} scheduled at {dt_str}.")
            self.close()
        except Exception as ex:
            QMessageBox.critical(self, "Schedule Error", str(ex))

class ScheduleManagementTab(QtWidgets.QWidget):
    """
    Displays RULE_SCHEDULES with CRUD operations: Refresh, Add, Update, Delete.
    """
    def __init__(self, connection, parent=None):
        super().__init__(parent)
        self.connection = connection
        layout = QVBoxLayout(self)

        self.table = QTableWidget(0, 6)
        self.table.setHorizontalHeaderLabels(["ScheduleID", "RuleID", "ScheduleTime", "Status", "RunVal?", "Actions"])
        self.table.horizontalHeader().setStretchLastSection(True)
        layout.addWidget(self.table)

        btn_layout = QHBoxLayout()
        refresh_btn = QPushButton("Refresh")
        refresh_btn.clicked.connect(self.load_schedules)
        btn_layout.addWidget(refresh_btn)

        add_btn = QPushButton("Add New Schedule")
        add_btn.clicked.connect(self.add_schedule)
        btn_layout.addWidget(add_btn)
        btn_layout.addStretch()
        layout.addLayout(btn_layout)
        self.setLayout(layout)
        self.load_schedules()

    def load_schedules(self):
        self.table.setRowCount(0)
        c = self.connection.cursor()
        try:
            c.execute("""
                SELECT SCHEDULE_ID, RULE_ID, SCHEDULE_TIME, STATUS, RUN_DATA_VALIDATIONS
                FROM RULE_SCHEDULES
                ORDER BY SCHEDULE_TIME DESC
            """)
            rows = c.fetchall()
            for row in rows:
                row_index = self.table.rowCount()
                self.table.insertRow(row_index)
                for col_index in range(5):
                    self.table.setItem(row_index, col_index, QTableWidgetItem(str(row[col_index])))
                # Action cell: update/delete
                action_widget = QWidget()
                action_layout = QHBoxLayout(action_widget)
                action_layout.setContentsMargins(0,0,0,0)
                update_btn = QPushButton("Update")
                update_btn.clicked.connect(lambda _, idx=row_index: self.update_schedule(idx))
                delete_btn = QPushButton("Delete")
                delete_btn.clicked.connect(lambda _, idx=row_index: self.delete_schedule(idx))
                action_layout.addWidget(update_btn)
                action_layout.addWidget(delete_btn)
                action_layout.addStretch()
                self.table.setCellWidget(row_index, 5, action_widget)
            self.table.resizeColumnsToContents()
        except Exception as ex:
            QMessageBox.critical(self, "Load Error", str(ex))

    def add_schedule(self):
        dlg = EnhancedScheduleDialog(self.connection, self)
        dlg.exec_()
        self.load_schedules()

    def update_schedule(self, row_index):
        item = self.table.item(row_index, 0)
        if not item:
            return
        sched_id = int(item.text())
        new_dt, ok = QInputDialog.getText(self, "Update Schedule", "New datetime (YYYY-MM-DD HH:mm:ss):")
        if not ok or not new_dt.strip():
            return
        c = self.connection.cursor()
        try:
            c.execute("UPDATE RULE_SCHEDULES SET SCHEDULE_TIME=? WHERE SCHEDULE_ID=?", (new_dt.strip(), sched_id))
            self.connection.commit()
            QMessageBox.information(self, "Updated", f"Schedule {sched_id} updated.")
        except Exception as ex:
            QMessageBox.critical(self, "Update Error", str(ex))
        self.load_schedules()

    def delete_schedule(self, row_index):
        item = self.table.item(row_index, 0)
        if not item:
            return
        sched_id = int(item.text())
        confirm = QMessageBox.question(self, "Confirm", f"Delete schedule {sched_id}?")
        if confirm != QMessageBox.Yes:
            return
        c = self.connection.cursor()
        try:
            c.execute("DELETE FROM RULE_SCHEDULES WHERE SCHEDULE_ID=?", (sched_id,))
            self.connection.commit()
            QMessageBox.information(self, "Deleted", f"Schedule {sched_id} removed.")
        except Exception as ex:
            QMessageBox.critical(self, "Delete Error", str(ex))
        self.load_schedules()
  

