Below are five common scenario-based SQL interview questions along with example answers and explanations. The exact SQL syntax may differ depending on your SQL dialect (e.g., SQL Server, MySQL, PostgreSQL), but the core logic remains similar.

1. Finding Top Customers by Purchase Amount

Scenario
You have two tables:
	•	Customers: contains customer_id, customer_name, and other customer details.
	•	Orders: contains order_id, customer_id, order_amount, and order_date.

You want to find the top 3 customers (by name) who have spent the most money overall (sum of their order_amount).

Question
Write a SQL query to list each customer’s total spending. Then retrieve only the top 3 spenders.

Answer (Example using a subquery and an analytic function or LIMIT)

-- Using a GROUP BY and ORDER BY + LIMIT (MySQL/PostgreSQL syntax)
SELECT 
    c.customer_name,
    SUM(o.order_amount) AS total_spent
FROM Customers c
JOIN Orders o ON c.customer_id = o.customer_id
GROUP BY c.customer_name
ORDER BY total_spent DESC
LIMIT 3;

Explanation
	1.	JOIN the Customers table with the Orders table on customer_id.
	2.	GROUP BY customer name to get the sum of all their order amounts.
	3.	ORDER BY the total amount descending to see the highest spenders first.
	4.	Use LIMIT 3 (or TOP 3 in SQL Server) to get the top 3 records.

2. Identifying and Deleting Duplicate Records

Scenario
You have a table called Users with the following columns:
	•	user_id (primary key, auto-increment)
	•	email
	•	full_name
	•	created_at

Some users were accidentally inserted multiple times, creating duplicate emails. You only want to keep the earliest record (the one with the smallest user_id or the earliest created_at) for each email, and remove the rest.

Question
Write a SQL statement (or series of statements) to delete the duplicate records, preserving only the earliest record.

Answer (Example using a common approach)

-- Approach 1: Using a self-join to identify duplicates (general SQL approach)
DELETE Users
FROM Users
JOIN (
    SELECT 
        email,
        MIN(user_id) AS min_user_id
    FROM Users
    GROUP BY email
) AS sub ON Users.email = sub.email
WHERE Users.user_id > sub.min_user_id;

Explanation
	1.	First, identify the minimum user_id for each email (the earliest record).
	2.	Join the main Users table back to this sub-result.
	3.	Delete rows whose user_id is greater than the minimum (which means they’re duplicates).

Note: Deletion of duplicates can differ by SQL dialect and constraints; always test in a safe environment before running in production.

3. Finding Employees Without Managers or With No Direct Reports

Scenario
You have an Employees table with these columns:
	•	employee_id
	•	employee_name
	•	manager_id (links back to an employee_id in the same table)
	•	hire_date

Part A: Find all employees who do not have a manager (i.e., manager_id is NULL).
Part B: Find all employees who are not a manager to anyone else.

Question
Provide queries that address both parts of the scenario.

Answer

Part A: Employees with no manager

SELECT 
    employee_id,
    employee_name
FROM Employees
WHERE manager_id IS NULL;

Part B: Employees who are not a manager

SELECT 
    e.employee_id,
    e.employee_name
FROM Employees e
WHERE e.employee_id NOT IN (
    SELECT manager_id
    FROM Employees
    WHERE manager_id IS NOT NULL
);

Explanation
	1.	Part A is straightforward: filter where manager_id is NULL.
	2.	Part B uses a subquery listing all manager_id values (excluding nulls), then selects all employees whose ID is not in that list. That means they never appear as someone else’s manager.

4. Calculating Monthly Sales with JOINs and Grouping

Scenario
You have three tables:
	1.	Orders: order_id, customer_id, order_date
	2.	OrderDetails: order_detail_id, order_id, product_id, quantity, price
	3.	Products: product_id, product_name, category

You want to find the total revenue per month (i.e., for each calendar month) over the last year.

Question
Write a query to calculate total monthly revenue (sum of quantity * price), grouping by month and sorted by the newest month first.

Answer (Example using date functions)

-- Example in SQL Server
SELECT 
    FORMAT(o.order_date, 'yyyy-MM') AS MonthYear,
    SUM(od.quantity * od.price) AS total_revenue
FROM Orders o
JOIN OrderDetails od ON o.order_id = od.order_id
WHERE o.order_date >= DATEADD(YEAR, -1, GETDATE())  -- last 12 months
GROUP BY FORMAT(o.order_date, 'yyyy-MM')
ORDER BY MonthYear DESC;

-- Example in MySQL
SELECT 
    DATE_FORMAT(o.order_date, '%Y-%m') AS MonthYear,
    SUM(od.quantity * od.price) AS total_revenue
FROM Orders o
JOIN OrderDetails od ON o.order_id = od.order_id
WHERE o.order_date >= DATE_SUB(CURDATE(), INTERVAL 12 MONTH)
GROUP BY DATE_FORMAT(o.order_date, '%Y-%m')
ORDER BY MonthYear DESC;

Explanation
	1.	We JOIN Orders with OrderDetails so we can multiply quantity * price.
	2.	We filter only orders in the last 12 months.
	3.	We GROUP by the month-year component of order_date.
	4.	We ORDER the results descending by the month-year so the newest month is on top.

5. Returning Records Where the Latest Status Applies

Scenario
You have a table TicketStatus that tracks the status of support tickets over time:
	•	ticket_id
	•	status (e.g., “Open”, “In Progress”, “Resolved”, “Closed”)
	•	updated_at (the timestamp when the status was set)

A ticket can appear multiple times, each time with a different status or update time. You need to retrieve only the most recent status for each ticket.

Question
Write a query to list each ticket’s most recent status and the time it was updated.

Answer (Example using ROW_NUMBER in SQL Server / Window Functions)

WITH LatestStatus AS (
    SELECT
        ticket_id,
        status,
        updated_at,
        ROW_NUMBER() OVER (PARTITION BY ticket_id ORDER BY updated_at DESC) AS rn
    FROM TicketStatus
)
SELECT
    ticket_id,
    status,
    updated_at
FROM LatestStatus
WHERE rn = 1;

Explanation
	1.	Use a window function (ROW_NUMBER() OVER (PARTITION BY ticket_id ORDER BY updated_at DESC)) to rank the statuses by the update time for each ticket.
	2.	The most recent status has a ROW_NUMBER of 1.
	3.	Select only rows where rn = 1, giving you the latest record for each ticket.

Final Tips
	1.	Understand the Data Model: Read the schema carefully—knowing how tables relate (primary/foreign keys) is crucial.
	2.	Use Proper Indexing: On large datasets, queries involving JOINS, GROUP BY, or ORDER BY can benefit significantly from correct indexing.
	3.	Test and Validate: Especially for DELETE or UPDATE statements, always validate logic in a non-production environment or by wrapping the statement in a transaction.



Below are eight advanced SQL interview questions suitable for professionals with 10+ years of experience. Each question focuses on complex or high-level SQL topics such as performance tuning, advanced joins, window functions, transactions/locking, and architectural considerations. These are the kinds of questions that probe deep expertise in SQL and database systems.

1. Advanced Performance Tuning and Query Optimization

Scenario
You have a large Orders table with hundreds of millions of rows. A certain SELECT query with multiple joins runs very slowly. You check the execution plan and see a full table scan. The table has an index on order_date and customer_id, but the query also filters on status and a complex subquery condition.

Question
How would you analyze and optimize this query step-by-step, ensuring it uses the most efficient plan possible? Include considerations around indexing, statistics, and rewriting parts of the query.

What to Look For in an Answer
	1.	Explain how to review the execution plan (e.g., using EXPLAIN in MySQL or Postgres, SHOWPLAN/SET STATISTICS in SQL Server) to identify problem areas (full scans, incorrect index usage, large sort/hash operations).
	2.	Discuss updating statistics and ensuring the optimizer has the right metadata to choose an index.
	3.	Partitioning strategies if the table is extremely large and you often filter by date.
	4.	Query rewrites: Possibly break a complex subquery into temporary tables, or use JOIN vs. IN/EXISTS if beneficial.
	5.	Index tuning: Multi-column indexes, covering indexes, or filtered indexes on status if it has high selectivity.

2. Handling Concurrency and Locking

Scenario
Your application frequently faces deadlocks or locking issues when multiple transactions run simultaneously. For instance, two transactions might update the same customer record but in a different order of operations, causing a deadlock.

Question
How do you troubleshoot and resolve deadlocks or locking issues in a high-throughput OLTP environment? What strategies can you use to minimize or avoid them?

What to Look For in an Answer
	1.	Explanation of deadlock detection (e.g., using DB system logs or built-in deadlock graphs in SQL Server).
	2.	Isolation levels: How “READ COMMITTED,” “REPEATABLE READ,” or “SNAPSHOT ISOLATION” impact locking.
	3.	Use of row versioning or optimistic concurrency to reduce write locks.
	4.	Transaction design: Keep transactions short, update tables in the same order, reduce hotspots.
	5.	Retry logic in the application to handle deadlock victims gracefully.

3. Complex Window (Analytic) Functions

Scenario
A marketing department wants to track each customer’s running total of purchases over time, plus the difference in purchase amount compared to the previous order. The table looks like this:

CustomerOrders
--------------
customer_id
order_id
order_date
amount

Question
How would you use window (analytic) functions to produce columns like:
	1.	running_total (cumulative sum of amount partitioned by customer_id, ordered by order_date), and
	2.	diff_from_previous (difference in amount from the prior order for that customer)?

Sample Answer

SELECT
    customer_id,
    order_id,
    order_date,
    amount,
    SUM(amount) 
        OVER (PARTITION BY customer_id
              ORDER BY order_date 
              ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_total,
    amount - LAG(amount, 1, 0) 
        OVER (PARTITION BY customer_id 
              ORDER BY order_date) AS diff_from_previous
FROM CustomerOrders;

What to Look For
	•	Understanding of PARTITION BY and ORDER BY clauses in window functions.
	•	Usage of LAG() or LEAD() to compare row values.
	•	Potential performance considerations (indexes on customer_id, order_date).

4. Partitioned Tables and Data Warehousing

Scenario
You’re designing a fact table in a data warehouse with billions of rows. You know users predominantly query on transaction_date.

Question
How do you decide on table partitioning strategy, and what are the considerations for how many partitions to create? Also, how do you load and maintain partitioned data efficiently?

What to Look For in an Answer
	1.	Partition keys selection: usually transaction_date or a derived period like “month” or “year-month.”
	2.	Potential pitfalls of having too many partitions or partitions that are too large.
	3.	Partition pruning: how the DB engine can skip entire partitions during queries.
	4.	Sliding window approach: how to remove old partitions or swap them out.
	5.	Indexing in a partitioned environment, including local vs. global indexes.

5. Design for High Throughput vs. Ad Hoc Analytics

Scenario
You’re given a system that must handle thousands of insert/updates per second (real-time transactions), but your analytics team also wants to run complex, long-running queries on the same database.

Question
How do you approach database and schema design to handle both OLTP (high throughput) and OLAP (complex analytical queries) without impacting performance significantly?

What to Look For in an Answer
	1.	Understanding of OLTP vs. OLAP workloads: row-store vs. column-store approaches.
	2.	Potential use of separate databases or replicas for analytics.
	3.	Use of change data capture (CDC) to feed a data warehouse or data lake.
	4.	Partitioning or archiving older data.
	5.	The concept of a Hybrid Transactional/Analytical Processing (HTAP) system if relevant, or advanced features like columnstore indexes (SQL Server, for instance).

6. Advanced Join Techniques and Pitfalls

Scenario
You need to perform a multi-table join across 5-6 tables. Some of these tables contain billions of rows. The developer’s initial approach times out or returns incorrect row counts due to accidental Cartesian products.

Question
What strategies do you use to ensure that multi-table joins are:
	1.	Producing correct results (avoiding duplicates or missing rows), and
	2.	Performing efficiently?

What to Look For in an Answer
	1.	Join types: Understanding of INNER vs. LEFT vs. RIGHT vs. FULL OUTER.
	2.	Use of explicit join conditions vs. WHERE filter to avoid cross joins.
	3.	Normalization and ensuring PK-FK relationships are correct.
	4.	Checking the query plan for large hash or merge joins, and possibly rewriting to smaller subqueries or temporary staging.
	5.	Considering index usage and how the DB can do index lookups vs. scans in a multi-join scenario.

7. Dynamic SQL and Stored Procedures

Scenario
A reporting system requires different aggregations and filters based on user input (e.g., multiple dimensions can be selected at runtime). The queries can become quite varied, making a single static SQL statement difficult to maintain.

Question
Explain how you would use dynamic SQL (e.g., building SQL strings within stored procedures) to handle variable filtering and grouping. What are the security and performance implications?

What to Look For in an Answer
	1.	How to construct a dynamic SQL string in a stored procedure or function with parameters.
	2.	Parameterization to prevent SQL injection or usage of sp_executesql in SQL Server.
	3.	Performance considerations: potential plan cache issues with dynamic SQL.
	4.	Possibly building a safe dynamic query generation approach (whitelisting or valid column checks).

8. Advanced Transaction Isolation and Consistency Models

Scenario
Your system handles financial transactions that must be ACID compliant. However, due to concurrency, you occasionally see anomalies like non-repeatable reads or phantom rows when analyzing data.

Question
Explain how different transaction isolation levels (READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE, SNAPSHOT) handle concurrency. How do you decide which isolation level is appropriate, and what are the trade-offs?

What to Look For in an Answer
	1.	Definitions of each isolation level (especially if using SQL Server or Oracle specifics).
	2.	Phantom reads, non-repeatable reads, and dirty reads—which isolation levels prevent them.
	3.	Overhead and performance trade-offs at higher isolation levels.
	4.	Potential usage of snapshot isolation to reduce writer blocking readers.
	5.	Real-world scenario: a financial system might require SERIALIZABLE or a robust approach to ensure data integrity.

Tips for Demonstrating Senior-Level SQL Expertise
	1.	Deep Knowledge of Query Plans: Be comfortable discussing how to read and interpret execution plans, identify hotspots like table scans, spool operations, or large sorts.
	2.	Indexing Strategy: Go beyond “create an index on this column.” Speak about composite indexes, covering indexes, filtered indexes, or partial indexes in Postgres.
	3.	Partitioning, Sharding, and Distribution: For very large data, discuss horizontal partitioning, sharding across servers, or using table partitioning to manage data volumes.
	4.	Database Internals: Show familiarity with how the query optimizer works, how cost-based decisions are made, and how statistics drive those decisions.
	5.	Real-World Examples: Cite experiences with production systems, concurrency issues, and performance bottlenecks you’ve tackled—and how you solved them.

These questions (and the depth of the answers) help interviewers gauge whether a candidate has the technical depth and real-world experience consistent with a senior (10+ years) SQL professional.


These scenarios demonstrate real-world SQL challenges such as combining data from multiple tables, handling duplicates, retrieving hierarchical data, grouping by time periods, and fetching the latest version of a record. They are common in interviews to test both SQL knowledge and problem-solving skills.